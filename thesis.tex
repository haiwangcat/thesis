\documentclass[12pt]{gatech-thesis}
\usepackage{amsmath,amssymb,verbatim,latexsym,float,epsfig,enumerate,array}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{caption}
\DeclareCaptionType{copyrightbox}
\usepackage{subcaption}
\usepackage{multicol}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
\usepackage[ruled,vlined, boxed]{algorithm2e}
\usepackage{color}
\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
\title{Automated synthesis for program inversion} %% If you want to specify a linebreak
                               %% in the thesis title, you MUST use
                               %% \protect\\ instead of \\, as \\ is a
                               %% fragile command that \MakeUpperCase
                               %% will break!
\author{Cong Hou}
\department{School of Computer Science}

%% Can have up to six readers, plus principaladvisor and
%% committeechair. All have the form
%%
%%  \reader{Name}[Department][Institution]
%%
%% The second and third arguments are optional, but if you wish to
%% supply the third, you must supply the second. Department defaults
%% to the department defined above and Institution defaults to Georgia
%% Institute of Technology.

\principaladvisor{Professor Richard Vuduc}
\committeechair{Professor Ignatius Arrogant}
\firstreader{Professor General Reference}[School of Mathematics]
\secondreader{Professor Ivory Insular}[Department of Computer Science and Operations Research][North Dakota State University]
\thirdreader{Professor Earl Grey}
\fourthreader{Professor John Smith}
\fifthreader{Professor Jane Doe}[Another Department With a Long Name][Another Institution]
%\setcounter{secnumdepth}{2}
\degree{Doctor of Philosophy}

%% Set \listmajortrue below, then uncomment and set this for
%% interdisciplinary PhD programs so that the title page says
%% ``[degree] in [major]'' and puts the department at the bottom of
%% the page, rather than saying ``[degree] in the [department]''

%% \major{Algorithms, Combinatorics, and Optimization} 

\copyrightyear{2013}
\submitdate{April 2013} % Must be the month and year of graduation,
                         % not thesis approval! As of 2010, this means
                         % this text must be May, August, or December
                         % followed by the year.

%% The date the last committee member signs the thesis form. Printed
%% on the approval page.
\approveddate{1 March 2013}

\bibfiles{example-thesis}

%% The following are the defaults
%%    \titlepagetrue
 \signaturepagefalse
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
%%    \bibpagetrue
\thesisproposalfalse
%%    \strictmarginstrue
%%    \dissertationfalse
%%    \listmajorfalse
%%    \multivolumefalse



\newcommand{\Drawgraph}[3]{
	\begin{figure}[H]
	\centering
	\begin{tikzpicture} [auto, >=stealth', scale=1]

	  \tikzstyle{array}=[rectangle, thick, minimum size=7mm, draw=black!75,font=\sffamily\small]
	  \tikzstyle{CFG}=[rectangle, thick, minimum size=7mm, draw=black!75,font=\sffamily\scriptsize,inner sep=0pt]
	  \tikzstyle{nothing}=[rectangle, thick, minimum size=7mm, draw=none]
	  \tikzstyle{scalar}=[circle, thick, minimum size=7mm, draw=black!75,font=\sffamily\tiny]
	  \tikzstyle{op}=[circle, , draw=black!75, fill=gray!25, minimum size=4mm, inner sep=0pt, font=\sffamily\tiny]
	   \tikzstyle{available}=[very thick]
	    \tikzstyle{reverse}=[dashed]
	    \tikzstyle{forward}=[densely dotted]
	  
 	 \tikzstyle{lbl}=[font=\sffamily]
  
	 #1
	
	\end{tikzpicture}
	\caption{#2}
	\label{#3}
	\end{figure}
}

\newcommand{\TODO}[1]{{\color{red}\textbf{#1}}}

\newcommand{\Code}[2]{
\begin{center}
    \begin{tabular}{ | p {6cm} | p {6cm} | }
    \hline
    Original Program & SSA Form \\ \hline
	$ \begin{aligned} 
	#1
	\end{aligned}  $
&
	$ \begin{aligned} 
	#2
	\end{aligned}  $
    \\ \hline
    \end{tabular}
\end{center}
}



\begin{document}
\bibliographystyle{gatech-thesis}


%%
\begin{preliminary}



% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.
\end{preliminary}
%%
\chapter{Introduction}


\newcommand{\naive}{na\"ive\xspace}
\newcommand{\Program}{\ensuremath{P}\xspace}
\newcommand{\Forward}{\ensuremath{\Program^{+}}\xspace}
\newcommand{\Inverse}{\ensuremath{\Program^{-}}\xspace}
\newcommand{\Input}{\ensuremath{I}\xspace}
\newcommand{\Output}{\ensuremath{O}\xspace}
\newcommand{\ExtraOuts}{\ensuremath{S}\xspace}
\newcommand{\OutsS}{\ensuremath{\Outs+\ExtraOuts}\xspace}
\newcommand{\Var}{\ensuremath{v}\xspace}
\newcommand{\Vars}{\ensuremath{V}\xspace}
\newcommand{\Exec}[4]{\ensuremath{{#1}\{{#2}={#3}\} \rightarrow \{{#2}={#4}\}}\xspace}


%\newcommand{\vmu}{\ensuremath{v_{in}^I}\xspace}
%\newcommand{\vinit}{\ensuremath{v_{in}}\xspace}
%\newcommand{\veta}{\ensuremath{v_{\eta}}\xspace}
%\newcommand{\vfinal}{\ensuremath{v_{out}}\xspace}
%\newcommand{\vmup}{\ensuremath{v_{\mu}'}\xspace}
%\newcommand{\viter}{\ensuremath{v_{out}^I}\xspace}
%\newcommand{\viterp}{\ensuremath{v_{iter}'}\xspace}
%\newcommand{\mufunc}{\ensuremath{v_{in}^I=\mu(v_{in},v_{out}^I)}\xspace}
%\newcommand{\etafunc}{\ensuremath{v_{out}=\eta(v_{in}^I)}\xspace}
\newcommand{\varmbox}[2]{\ensuremath{{#1}_{\tiny\mbox{#2}}}}
\newcommand{\vmu}{\ensuremath{\varmbox{v}{in}^I}\xspace}
\newcommand{\vinit}{\ensuremath{\varmbox{v}{in}}\xspace}
\newcommand{\veta}{\ensuremath{\varmbox{v}{\eta}}\xspace}
\newcommand{\vfinal}{\ensuremath{\varmbox{v}{out}}\xspace}
\newcommand{\vmup}{\ensuremath{\varmbox{v}{\mu}'}\xspace}
\newcommand{\viter}{\ensuremath{\varmbox{v}{out}^I}\xspace}
\newcommand{\viterp}{\ensuremath{\varmbox{v}{iter}'}\xspace}
\newcommand{\mufunc}{\ensuremath{\varmbox{v}{in}^I=\mu(v_{in},v_{out}^I)}\xspace}
\newcommand{\etafunc}{\ensuremath{\varmbox{v}{out}=\eta(v_{in}^I)}\xspace}
\newcommand{\Loop}{\ensuremath{L}\xspace}



In this thesis, we consider the problem of synthesizing program inverses for imperative languages. Our primary motivation comes from optimistic parallel discrete event simulation (OPDES). There, a simulator must process events while respecting logical temporal event-ordering constraints; to extract parallelism, an OPDES simulator may speculatively execute events and only rollback execution when event-ordering violations occur. In this context, the ability to perform rollback by running time- and space-efficient reverse programs, rather than saving and restoring large amounts of state, can make OPDES more practical. Synthesizing inverses also appears in numerous other software engineering contexts, such as debugging, synthesizing “undo” code, or even generating decompressors automatically given only lossless compression code.This thesis mainly contains three chapters. In the first chapter, we focus on handling programs with only scalar data and arbitrary control flows. By building a value search graph (VSG) that represents recoverability relationships between variable values, we turn the problem of recovering previous values into a graph search one. Forward and reverse programs are generated according to the search results. For any loop that produces an output state given a particular input state, our method can synthesize an inverse loop that reconstructs the input state given the original loop's output state. The synthesis process consists of two major components: (a) building the inverse loop's body, and (b) building the inverse loop's predicate. Our method works for all natural loops, including those that take early exits (e.g., via breaks, gotos, returns). In the second chapter we extend our method to handling programs containing arrays. Based on Array SSA, we develop a modified Array SSA from which we could easily build equalities between arrays and array elements. Specifically, to represent the equality between two arrays, we employ the array subregion as the constraint. During the search those subregions will be calculated to guarantee that all array elements will be retrieved. We also develop a demand-driven method to retrieve array elements from a loop, in which each time we only try to retrieve an array element from an iteration if that element has not been modified in previous iterations. To ensure the correctness of each retrieval, the boundary conditions are created and checked at the entry and the exit of the loop. In the last chapter, we introduce several techniques of handling high-level constructs of C++ programs, including virtual functions, copying a C++ object, C++ STL containers, expressions with several side effects, inter-procedural function calls, etc. Since C++ is an object-oriented (OO) language, our discussion in this chapter can also be extended to other OO languages like Java.We have implemented our algorithm as part of a compiler framework named Backstroke, a C++ source-to-source translator based on ROSE compiler. Experimental results show that our method is effective and produces better performance than previously proposed methods.









We consider the problem of synthesizing \emph{program inverses}.
That is, given a program \Program with input state \Input and output state \Output, its \emph{inverse} or \emph{reverse program}, \Inverse, produces \Input given \Output.
Our primary motivation comes from optimistic parallel discrete event simulation (OPDES).
There, a simulator must process events while respecting logical temporal event-ordering constraints;
to extract parallelism, an OPDES simulator may speculatively execute events and only \emph{rollback} execution when event-ordering violations occur~\cite{Jefferson1985}.
In this context, the ability to perform rollback by running time- and space-efficient \Program and \Inverse, rather than saving and restoring large amounts of state, can make OPDES more practical.
%
Synthesizing inverses also appears in numerous other software engineering contexts, such as debugging~\cite{Biswas1999}, synthesizing ``undo'' code, or even generating decompressors automatically given only lossless compression code~\cite{Srivastava2011}.
The challenge in any of these contexts is that constructing program inverses manually is a tedious, time-consuming, and error-prone task.

%Running the original \Program followed immediately by \Inverse is equivalent to a no-op.
The program \Program will generally contain non-invertible statements, such as a destructive assignment.
However, in these cases it may still be possible to create an inverse.
In particular, we may create an instrumented version of \Program, called the \emph{forward program}, \Forward, with semantics-preserving changes so that it becomes possible to construct \Inverse from \Forward.
We then replace executions of \Program with \Forward.
For instance, suppose \Program overwrites a variable \Var.
We may construct \Forward so that it saves the value of \Var prior to overwriting it.
Then, \Inverse need only restore the saved value to recover \Var's prior value.
In this case, \Forward produces extra outputs, which we denote by \ExtraOuts.
Indeed, even if \Program is theoretically reversible without requiring extra output, we may nevertheless need to generate \ExtraOuts due to fundamental technical limits on program analysis.

In this thesis, we will introduce our system that generates \Forward and \Inverse for a given \Program. 
Specifically, we categorize \Program into four types: \Program with only scalars and without loops; \Program with only scalars and with loops; \Program with arrays and without loops; \Program with arrays and with loops.
We will first introduce how we handle the first three cases using a bunch of compilation techniques.
Handing programs with arrays in loops is exactly the proposed work, being introduced in the last chapter.


\section{Related work}
\label{sec:related-work}
Most of the work on inverting arbitrary (non-injective) imperative programs has focused an incremental approach: the imperative program is essentially executed in reverse, with each modifying operation in the original execution being undone individually.
For example, if statements $s_{1} s_{2} \dots s_{n}$ are executed in the forward directions, the reverse function executes statements $s_{n}^{-1} \dots s_{2}^{-1} s_{1}^{-1}$.
The incremental approach cannot handle unstructured control flows and is difficult to apply with early returns from functions; the approach presented in this thesis suffers from neither of these shortcomings.  
Furthermore, the incremental inversion restores the initial state by restoring every intermediate program state between the final state and the initial state, even though these states are not needed

Among the incremental inversion approaches, syntax-directed approaches apply only statement-level analysis. 
If an assignment statement is lossless, its inverse is used: for example, the inverse of an integer increment is an integer decrement.
Otherwise, the variable modified in the assignment has to be saved.
An early example of syntax-directed incremental inversion is Brigg's Pascal inverter \cite{Briggs1987}. 
This approach was later extended to C and applied both to optimistic discrete event simulation \cite{Carothers1999} and reversible debugging \cite{Biswas1999}.

Akgul and Mooney introduced a more sophisticated incremental inversion algorithm that uses def-use analysis to invert some assignment statements that are not lossless \cite{Akgul2004a}; we refer to this approach as regenerative incremental inversion.
In order to reverse a lossy assignment to the variable $a$, such as $a \leftarrow 0$, the regenerative algorithm looks for ways to recompute the previous value of $a$. 
One technique to obtain the previous value of $a$ is to re-execute its definition; another technique is to examine all the uses of $a$ and see if its value can be retrieved from any of its uses. 
These two techniques are applied recursively whenever a modifying operation is to be reversed; if they fail to produce a result, the overwritten variable is saved during forward execution.
Our approach takes advantage of all the def-use relationships utilized by regenerative inversion, without suffering from the drawbacks of incremental inversion. 
In addition to def-use information, our approach  also derives equality relationships between variables from the outcome of branching statements that test for equality or inequality.

A related line of work is inverting programs that are injective, without using any state saving.
Most such work focuses on inverting functional programs \cite{Abramov2002a,Gluck2005,Kawabe2005}.
Approaches to inverting imperative programs include translation to a logic language \cite{Ross1997} and template-based generation of inverse candidates using symbolic execution \cite{Srivastava2011}.


\section{Limitations}

Most discussions in this thesis don't target any specific languages, but our method has several limitations.

\paragraph{Aliasing} Aliasing commonly exists in almost all imperative languages.
In C++, aliases are brought by pointers and references, where specific data may be referenced by more than one pointers or references, and at compile time it is usually very difficult or even impossible to resolve all those aliases.
In addition, the inter-procedural analysis is also heavily depending on aliasing analysis, as procedure calls usually have parameters passed by references.

Our method heavily depends on an intermediate representation called \emph{Static Single Assignment (SSA)}, and it is difficult to build it for programs with aliasing. 
Our compiler also lacks of well formed aliasing analysis framework.
Therefore, we won't discuss aliasing in this thesis and will assume there is no aliasing in the programs we handle. 

\paragraph{Subset of C++ language} 

\section{Preliminaries}

\subsection{Static Single Assignment Form (SSA)}

SSA is a commonly used intermediate representation in compilers to speed up data flow analysis \cite{Cytron1991}. 
For programs in SSA form, variables are versioned so that each variable with a specific version has exactly one reaching definition. 
Where distinct definitions of a variable merge at confluence points in the control flow graph (CFG), a $\phi$ function is introduced to merge each of the reaching definitions at that point, and this $\phi$ function is also treated as a new definition. SSA has been successfully applied to many compiler analyses. 

\subsection{SSA Graph}

 An SSA graph \cite{Alpern1988}\cite{Cooper2001}, built based on SSA form, consists of vertices representing operators, function symbols, or $\phi$ functions, and directed edges connecting uses to definitions of values. It shows data dependences between values in a program.



\chapter{Synthesis for programs with only scalars}


%============================================================
\section{Handling loop-free programs}
\label{sec:scalar-loop-free}
\input{scalar-loop-free}

%============================================================
\section{Handling programs with loops}
\label{sec:scalar-loops}
\input{scalar-loops}


%============================================================
%============================================================


\chapter{Synthesis for programs with arrays}
\label{chapter:arrays-loop-free}

\TODO{To be revised}
In this chapter, we extend the previous method to handling arrays in both loop-free programs and those with loops.

There are two motivations of it: in optimistic  parallel discrete event simulation, if we can avoid performing state saving on an array that is modified in a loop and can be retrieved by another loop in the reverse program, the performance will be improved; 
we also consider to generate the inverse for injective programs, such as lossless compression and encryption,  when state saving is not allowed (because the program is already injective or reversible, it is not necessary to generate a forward program). 

Arrays are a particularly important case: naively saving the entire array too frequently will be very expensive in space and time. Our method permits saving of just the elements that have changed, and better yet, can often use cheaper computation in place of state saving.

To build the VSG for arrays, we apply a modified Array SSA based on \cite{}, and define several special VSG nodes for arrays. 
To represent the equality between two arrays, we employ the array subregion as the constraint. 
During the search those subregions will be calculated to guarantee that all array elements will be retrieved. We also develop a demand-driven method to retrieve array elements from a loop. 

%============================================================
\section{Handling loop-free programs with arrays}
\label{sec:array-loop-free}
\input{arrays}

%============================================================
\section{Handling programs with loops and arrays}
\label{sec:array-loops}
\input{array-loops}



\begin{comment}
\chapter{Synthesis for programs with arrays}

Usually data-flow analysis for arrays is more difficult than that for scalars. 
Unlike an scalar, each element in an array does not have a static unique name, but is represented as an array name and an index. 
Two variables as indices of the same array with the same value bring aliases to the indexed element, and this aliasing is difficult to be detected at compile time. 
Fortunately, with the help of the Array SSA \cite{Rus} proposed for programs with arrays, we could build the VSG for such programs and develop a searching rule in order to generate a valid RG. 
The Array SSA tries to match definitions and uses of partial array regions, and we will also employ such array regions in our VSG as another condition of equalities between arrays (the condition we previous used is  control flow path set). 
As a result, we can still reuse the whole framework we previously proposed  to handle programs with arrays.

In this chapter, we first introduce the Array SSA, then describe how we make use of it and add array nodes and edges in the VSG. Then we still divide the programs with arrays into two categories: programs without and with loops. 
We will show how we handle loop-free programs, and propose a method to deal with more challenging programs with loops.
%In this chapter we will use an Array SSA proposed in [] in which concrete array regions are employed.
%Based on this Array SSA, we could build the VSG with special array nodes inside.
%By modifying the searching rules on the VSG, we will see that we could handle programs with arrays using the same framework as before. 


\section{Array SSA}

In the original SSA, when an element of an array is modified, the array itself is assigned with a new version, and the whole array is killed by that definition. 
In Array SSA, the definition of an array element only kills the previous definition of that element represented by a subregion but not the whole array. 
This new SSA form representation can accurately represent the use-def relations between array regions. 

There are three special $\phi$ functions defined for arrays: a $\delta$ function accounts for a partial kill after an element of the array is defined; a $\mu$ function is defined at the beginning of the loop header just as it for scalars; a $\eta$ function defined at the exit of a loop as the output of the loop. 
%a $\gamma$ function defined at the join node in CFG joining definitions of arrays from different control flow paths.

%there is one special $\phi$ function (called a $\delta$ function) added just after the definition, from which a partial kill is represented. For example:


\subsection{$\delta$ function}
 
 When an array element  is modified, except renaming the array's name (assigning it with a new version), a $\delta$ function is defined just after the definition to that element. 
The $\delta$ function summarizes the data-flows of all elements in the array and builds accurate def-use relations with the help of array regions.
 The syntax of a $\delta$ function is shown below:
 $$[a_n, @a_n] = \delta(a_0, [a_1, @a_1^n], [a_2, @a_2^n],  ... , [a_m, @a_m^n] )$$
 $$where \; @a_n=\bigcup_{k=1}^m@a_k^n \; and \; @a_i^n \cap a_j^n = \emptyset, \forall 1\le i,j \le m, i \ne j$$

In the equation above, $@a_k^n$ represents the array region in which the definitions of all elements of $a_k$ are not killed before the definition of $a_n$.
 The code below shows two $\delta$ functions after the definitions of two array elements respectively.
 \begin{flalign*} 
& int \; a_0[N], i_0, j_0; \\
& a_1[i_0] = ...;\\
& [a_2, @a_2] = \delta (a_0, [a_1, @a_1^2]); \\
& a_3[j_0] = ...;\\
& [a_4, @a_4] = \delta (a_0, [a_2, @a_2^4], [a_3, @a_3^4]); 
\end{flalign*} 
where $@a_1^2=@a_2=\{i_0\}, @a_2^4=\{i_0\}-\{j_0\}, @a_3^4=\{j_0\}, @a_4=\{i_0\}\cup\{j_0\}$. Note each array region is represented by a set including symbols (constants or SSA names). We will discuss the representation of array regions later.

In this example, after the definition in the second line, there is a $\delta$ function that defines a new version of the array $a$ as $a_2$. 
%In addition, each argument of the $\delta$ function is a definition of the array with a region. 
The argument $ [a_1, @a_1^2]$ of the $\delta$ function means that all elements of $a_2$ in the region $@a_1^2=\{i_0\}$ are defined by $a_1$. 
The first argument $a_0$ has the definitions of the elements in the region not belonging to other arguments.
 %Each argument except the first one is a tuple $[a_n, @a_n]$ including a definition of the array and the corresponding array region. 
% All elements in this region The $\delta$ function summarizes the reaching definitions of all elements: after one of them is redefined: 

%In this example, after the definition in the second line, there is a $\delta$ function that defines a new version of the array $a$ as $a_2$. In $a_2$, only the value indexed by $i$ equals that in $a_1$, and any other value equals the corresponding one in $a_0$. This means only the $i$th value of $a_0$ is killed by the definition of $a_1$.


\subsection{$\mu$ and $\eta$ functions}

Preiously when we handle loops with scalars, we introduced a $\mu$ function for each variable modified in the loop. 
The arguments in the $\mu$ function come from outside and inside of the loop, and each definition of the $\mu$ function kills all previous definitions. 
In Array SSA,  for each array modified in the loop we also define a $\mu$ function in the loop header receiving definitions from outside and inside of the loop. However, like a $\delta$ function, a $\mu$ function only kills the definition in some regions of the array, and the region is related to the loop index. Let $i$ be the loop index, then all regions in a $\mu$ function are functions of $i$. The syntax of a $\mu$ function is defined by the following equation.
 $$[a_n, @a_n] = \mu(a_0, (i=1,p), [a_1, @a_1^n(i)], [a_2, @a_2^n(i)],  ... , [a_m, @a_m^n(i)] )$$

In this equation the sets $@a_k^n(i)$ are functions of the loop index $i$ and they represent the sets of memory locations defines in some iterations $j<i$ by definition $a_k$ and not killed before reaching the beginning of iteration $i$. 
For any array element defined by $a_k$ in some iteration $j < i$, in order to reach iteration $i$, it must not be killed by other definitions to the same element. 
There are two kinds of definitions that will kill it: definitions ($Kill_s$) that will kill it within the same iteration $j$ and definitions ($Kill_a$) that will kill it at iterations from $j +1$ to $i-1$.
The definition of $@a_k^n(i)$ is shown below.
 $$@a_k^n(i)=\bigcup_{j=1}^{i-1}\left[@a_k(j)-\left(Kill_s(j) \cup \bigcup_{l=j+1}^{i-1}Kill_a(l)\right)\right]$$
 $$ where \; Kill_s = \bigcup_{h=k+1}^m @a_h, \; and \; Kill_a=\bigcup_{h=1}^m@a_h$$

%\section{Arrays in the VSG}

\section{Handling arrays in loop-free programs}

\subsection{Array region representation}

%Unless explicitly indicated, when we retrieve an array, we are retrieving all elements of it. 
Given a C/C++ style array $a[N]$ with length $N$, then all elements can be represented by an interval $[0,N-1]$, or $[0:N-1]$. 
We will use this interval as a set that includes all integers in that interval, and hence an array region becomes an integer set.
%Here $[0,N)$ represents a set containing all integers in this interval. 
%We will use this representation through this report. 
$[0:N-1]$ represents the \emph{universal set} of all regions of the array $a$, denoted by $\mathcal{A}(a)$. 
%Instead of using $@a$, we use $R$ to represent a region in an array. 

Now we define some basic array regions that are commonly used later:
%Among all its subsets, there are some special ones: 

\begin{itemize}
\item The region containing only one integer is represented by $\{i\}$, where $i$ is a constant or an SSA name. 
\item The complementary set $\overline{\{i\}} = \mathcal{A}(a)-\{i\}$ represents all other elements except the $i$th one. 

\item The triplet representation $[a:b:c]$ represents all integers between $a$ and $c$ with stride $b$. If $b=1$, it can be simplified as $[a:c]$. 

\end{itemize}

All set operations can be performed on array regions. Actually, we will rely on those operations to determine if all elements of an array are retrieved during the search on the VSG. 
In order to build a more accurate RG, we need to simplify an expression with set operations.
The process of simplification  depends on the properties of set operations, including identity laws, domination laws, idempotent laws, commutative law, associative laws, distributive laws, etc..
In addition, since an array region set may contain SSA names. 
The knowledge of the relations between them can also help to simplify an set expression.
For example, $\{i\} \cup \{j\} = \{i\} =\{j\}$ if $i=j$, and $\{i\} \cap \{j\} = \emptyset$ if $i\ne j$.
%For example, $\{i\} \cup \overline{\{i\}} = [0,N)=U$. 

%For induction variables in a loop (assume it has depth one), we use two regions for arrays indexed by an induction variable: one for the local iteration, and one for a summary for the scope outside of the loop. 

\subsection{Arrays in the VSG}

For each definition of an array we add a special node in the VSG as its representation and we call it an \emph{array node}. 
Any element of an array is a scalar and is still denoted by a normal value node. 
%Remember that every edge in a VSG is attached with the path information.
For any edge between two array nodes $a_x$ and $a_y$, an array region $R$ is attached to it, showing that $a_x$ and $a_y$ have the same values for all elements in the region $R$.
For each access of an array, e.g. $a_x[i]$, we also add a region $\{i\}$ on the edge between VSG nodes representing $a_x$ and $a_x[i]$.

We will build the VSG for arrays based on the Array SSA with some modifications. 
%Let's see first how we build arrays nodes for a $\delta$ function. 
In Array SSA, a $\delta$ function is always defined under a definition to an element of it, and those two definitions create two names of the same array. 
To reduce the number of array nodes in VSG, we combine those two names into one.
%, and connect this array node to other array nodes and the definition of its element. 
For example, for the following code:
\begin{flalign*} 
& int \; a_0[N]; \\
& a_1[i] = ...;\\
& [a_2, @a_2] = \delta (a_0, [a_1, @a_1^2]); \\
& a_3[j] = ...;\\
& [a_4, @a_4] = \delta (a_0, [a_2, @a_2^4], [a_3, @a_3^4]); 
\end{flalign*} 
We will transform it into the code below:
\begin{flalign*} 
& int \; a_0[N]; \\
& a_1[i] = ...;\\
& a_1 = \delta ([a_0, \overline{\{i\}}], [a_1, \{i\}]);  \\
& a_2[j] = ...;\\
& a_2 = \delta ([a_1, \overline{\{j\}}], [a_2, \{j\}]); 
\end{flalign*} 

After the definition of $a_1$, instead of creating a new name $a_2$, we maintain a correct equality relations between $a_1$ and $a_0$, and also $a_1$ and $a_1[i]$. 
In the VSG, we  add an edge with region $\overline{\{i\}}$ between nodes of $a_0$ and $a_1$, and  add another edge with region $\{i\}$ between node of $a_1$ and $a_1[i]$. 
With the help of this region information, we can make a full kill to $a_0$ when $a_1$ is defined (that is, $a_0$ will never be used after the definition of $a_1$). 
Similarly, when $a_2[j]$ is defined, we create another $\delta$ function that shows the equality between $a_1$ and $a_2$ in the region $\overline{\{j\}}$, and $a_2$ and $a_2[j]$ in the region $\{j\}$.
In this way, a $\delta$ function always has two arguments: one is the newly defined array with the region $\{i\}$ where $i$ is the index of the defined element; one is the reaching definition of the array before defining the element, with the region $\overline{\{i\}}$.
%As a consequence, all later use of $a_2$ will be replaced by $a_1$.
The created nodes and edges in the VSG are shown below:


\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=above:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \path
    (a1) edge node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge node  [lbl, swap] {$\overline{\{j\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    ;
}{The VSG edges with array regions between array nodes.}{fig:array-vsg}

In the VSG, each array node is represented by a square node, being differentiated from circle nodes for scalars. 
For each element access of an array $a_x[i]$, the corresponding value node will be connected to the array node for $a_x$ with region $\{i\}$.
%The region informations are attached to edges incident to array nodes.

In Array SSA,  $\phi$ functions\footnote{In the original paper of Array SSA, it is called a $\gamma$ function.} defined for arrays at a join node in the CFG has the same meaning as those for scalars.
%there is also a for an array if it has different definitions from incoming edges. In this case, we take it in the same way as scalars. 
Therefore, in the VSG we build a $\phi$ array node representing the array defined by the $\phi$ function and connect it to all of its arguments with correct control flow path set and  the full array region. 






%Note that an array can only be modified through each of its elements.

\subsection{Search the VSG to retrieve an array}

The searching rules for arrays on the VSG is similar to that for scalars, except we have to take the array region into account.
In the VSG, every edge incident to an array node has both control path information and array region information. 
During the search, once such an edge is selected, the search begins to have an addition goal to retrieve all elements of the corresponding array in the region on that edge.
Only when the search reaches available array nodes or scalar value nodes can the region goal added to the search be removed.
At the beginning of the search, if an array node is the target node, then the search needs to retrieve all elements of the array.
%The special searching rules for arrays:

During the search on the VSG, it is possible that an edge is selected several times in searching for more than one values with different control flow paths and array regions. 
Therefore, in the RG, the control flow path and array region sets are paired as $[P,R]$, where $P$ and $R$ stand for the path set and array region set respectively.
 And it is possible that an edge $e$ has several such pairs $[P_0,R_0],[P_1,R_1],...,[P_n,R_n]$.
We require that $P_i \cap P_j = \emptyset, i\ne j$.
Then for the edge $e$ we can get those sets through $P_i(e)$ or $R_i(e), i = 0,...,n$.

As previously, the searching rule should guarantee the search result RG to meet several properties. 
For each edge $e$ in the RG, let $P(e)$ denote the control flow path set on it, and let $R(e)$ denote the array region set on it.
Those properties include:

\begin{enumerate}[I)]

\item For each target array node $n$ of the array $a$, and for each control flow path $p$,
	$$\bigcup_{\substack{\mathit{out} \in \text{OutEdges}(n)  \\  p \in P_i(out)}}R_i(\mathit{out}) \quad = \quad \mathcal{A}(a) $$ 
	\label{rg-property-1}

\item For each array node $n$ that is neither a target node nor an available node, and for each control flow path $p$,
	$$\bigcup_{\substack{\mathit{out} \in \text{OutEdges} (n) \\ p \in P_i(out)}} R_i(out) = \bigcup_{\substack{\mathit{in} \in \text{InEdges} (n)  \\  p \in P_j(in) }}R_j( \mathit{in} )$$ 

\item For each array node $n$, given any two outgoing edges $n\to s$ and $n\to t$, for each control flow path $p$, and $p \in P_i(n \to s)$, and $p \in P_j(n \to t)$, then $R_i(n\to s) \cap R_j(n\to t) = \emptyset$.

\item If $e$ is a route graph edge and its corresponding edge in the VSG is $e'$, then for each $R_i(e)$,
$R_i(e) \subseteq R(e')$.
	\label{rg-property-4}

%\item For each directed cycle with edges $e_1 \dots e_n$,  $\quad \bigcap_{i=1}^n{P(e_{i})} = \emptyset$
	%\label{rg-property-5}

\end{enumerate}




%1. For each edge incident to an array node, except the path information, there is also region information. 

%2.  If the search starts from an array node, it tried to retrieve all elements of that array. Then the region $[0,N)$ is attached to this search. This is very similar to the path information. 

%3. When the search reaches an element node from an array node, the region of the search can be removed. The search from node a to node b has the region information iff a or b is an array node.

%4. In the RG, for each array node, R(in)=R(out).


\subsection{State saving on an array and its elements}

In the VSG, to enable the capability of generating state saving statements in forward program, there is still a state saving node, which is connected to all scalar value nodes as well as array nodes. 
Each state saving edge connected to an array nodes of the array $a$ has the region   $\mathcal{A}(a)$.
When the search reaches a state saving edge incident to an array node, the array region on it will be calculated and updated according to the searching rule. 
As a result, it is possible that the final region on this state saving edge is unknown at compile time or is represented by an expression with several set operations.
To facilitate the code generation, we require that the region on a state saving edge must fall into two categories: a single element, or the full region. 
In the first case, we only need to store a single element of the array; in the second case, we will store the whole array by building a loop, and the size of the loop needs to be retrieved before storing the whole array. 


\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=below:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \node [scalar] (ss) at (9,0) {SS};
    \path
    (a1) edge node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge node  [lbl, swap] {$\overline{\{j\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    (a0) edge [bend left] (ss)
    (a1) edge [bend left] (ss)
    (a1i) edge (ss)
    (a2j) edge (ss)
    ;
}{A state saving node connecting all value nodes.}{fig:array-ss-vsg}

\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=below:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \node [scalar] (ss) at (9,0) {SS};
    \path
    (a1) edge [pre, red,ultra thick] node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge [pre, red,ultra thick] node  [lbl, swap] {$\overline{\{j\}} \cap \overline{\{i\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    (a0) edge [post, red,ultra thick,bend left] node  [lbl] {$\{i\}$} (ss)
    (a1) edge [post, red,ultra thick,bend left] node  [lbl,swap] {$\{j\}\cap \overline{\{i\}}$} (ss)
    (a1i) edge (ss)
    (a2j) edge (ss)
    ;
}{The search result is shown in bold red edges.}{fig:array-ss-rg}


For example, Figure~\ref{fig:array-ss-vsg} shows the VSG for the code we used before, and Figure~\ref{fig:array-ss-rg} is the search result RG shown as bold edges.
The array region on the edge from $a_1$ to $SS$ node is $\{j\}\cap \overline{\{i\}}$.
And $\{j\}\cap \overline{\{i\}} = \left\{ \begin{array}{rcl}  \{j\} & \mbox{if} & i \ne j \\ \emptyset & \mbox{else} \end{array}\right.$. 
If we cannot resolve the result of it at compile-time (that it, we don't know if $i$ and $j$ have the same value), we will use $\{j\}$ instead of $\{j\}\cap \overline{\{i\}} $ in the RG.
In the generated code, we always perform a state saving on $a_1[j]$.

\end{comment}

%\section{Handling arrays with loops}

%\section{Handling object access}

%\section{Handling linked data structure}

%This is what to be proposed in this proposal.


\section{Handling linked data structures}
\label{linked-data-structure}

A linked data structure contains elements which have one or more ``links'' to other elements such that each element could exist in separate memory place from others, comparing to arrays whose elements are always stored together.
Typical linked data structures include linked list, tree, graph, etc..
Since an element can be linked by several links from other elements, aliasing prevalently exists among linked data structures.

If we treat links as indices in arrays, it is possible to handle those linked data structures using the method we developed for arrays.
Let's take the linked list (we will call it as list briefly) for an example.
Let $Node$ define a node in the list and its definition is shown below:


\begin{lstlisting}
struct Node {
    int val;
    Node* next;
};
\end{lstlisting}

 \begin{flalign*} 
 & struct \;  Node \; \{ \\
 & \;\;\;\;  int \; val; \\
 & \;\;\;\; Node* \; next; \\
 & \}; \\
 & Node* \; head; \\
 & while\; (head \; != \; NULL)\;\{ \\
 & \;\;\;\; head->val++;\\
 &\;\;\;\; head = head->next;\\
 &\}
 \end{flalign*} 

In the code above, we can treat $val$ and $next$ as two arrays, then the loop will be transformed as the code below:
 \begin{flalign*} 
 & while\; (head \; != \; NULL)\;\{ \\
 & \;\;\;\; val[head]++;\\
 &\;\;\;\; head = next[head];\\
 &\}
 \end{flalign*} 
 
However, unlike the index of an arrays that may have regular access patterns, we usually don't have enough information of the pointers pointing to linked data structures. 
We will try to find out in which situation or under what conditions can we handle them.




\chapter{Synthesis for C++ programs}

In prior chapters, we target programs in no specific languages, as our method is general and can be applied on any imperative language.
In this chapter, we introduce some techniques of handling several high-level constructs of C++ programs. 
We choose C++ for three reasons:
first, Backstroke is based on ROSE compiler, which is a C++ source-to-source compiler; 
second, C++ is an object-oriented (OO) language, and our discussion in this chapter can also be extended to other OO languages like Java;
third, initially Backstroke is used to generate reverse functions for OPDES events, and our  simulation engines (GTNets and ROSS) are writing in C/C++.

\section{Normalizing C++ programs}

Backstroke is a source-to-source translator.
It is based on ROSE,  which is a C++ source-to-source compiler. 
The ROSE compiler parses the source code and transform it into an abstract syntax tree (AST) as the intermediate representation (IR).
Since the target code is also source code, the AST preserves almost all syntax informations on source level of the source code, such that it can reproduce the similar code as the input together with the transformations as desired.
But it is not sufficient for some low level analysis, where other IRs like three address code will do better work.
%However, no further IR like three address code is generated after that, making it difficult to build some constructs relying on low level IRs. 

Our method heavily depends on SSA form, and building it on source code is challenging, as the source may contain complex statements or expressions.
In addition, because the forward program is generated by instrumenting state saving and path recording statements in the original program, normalizing the program can make it easier to find those instrumentation locations in the program. 
%Therefore, before we generate the SSA form, we first normalize the AST into a regular one.

\subsection{Forcing a specific execution order of several expressions}
%1. If the execution order of two expressions are not defined, we force a specific order.

In C++, there is a concept called \emph{sequence point} that is used to indicate the execution order of several expressions. 
A sequence point defines any point in a computer program's execution at which it is guaranteed that all side effects of previous evaluations will have been performed, and no side effects from subsequent evaluations have yet been performed.
Normally, all expressions in a statement are executed before the next statement. 
But in one statement, several expressions with side effects may exist. 
In this case, the C++ standard  only guarentees the execution order for the following expressions:

\begin{itemize}

\item Comma expression: $expr1, expr2$. 
In this expression, $expr1$ must be executed before $expr2$, and the returned value of $expr2$ is used as the returned value of this comma expression.

\item Logical and/or operator: $expr1 \;\&\&\; expr2$ / $expr1\; ||\; expr2$. 
In this expression, $expr1$ must be executed first, and $expr2$ is  executed only if the returned value of $expr1$ is $true$/$false$ for logical and/or operations.
This is called \emph{short-circuit evaluation}.

\item Conditional operator: $expr1 \;?\; expr2 : expr3$. In this expression, $expr1$ will be executed first, and if the returned value is $true$, then $expr2$ is  executed; otherwise, $expr3$ is executed.

\end{itemize}



\section{State savings on C++ variables}

In the prior chapters, when we need a state saving, we normally call two functions: $store$ and $restore$.
But what is behind those two function calls?
In this section, we will introduce how and where to store variables in Backstroke.
Besides variable of basic types, we will discuss how to store a C++ object correctly, including the case where polymorphism exists.

\subsection{The storage stack}

To perform state savings, we need a space and also some interfaces to perform the storing and restoring operations. 
Since Backstroke performs incremental state savings, the names and number of variables to be stored and the size of space needed are unknown at compile time.
Therefore, we have to use a runtime data structure as the storage.
In Backstroke, we use a stack that stores all variables in all programs.
Since stack is a LIFO (last-in-first-out) data structure, a variable that is firstly saved and pushed will be popped at last. 
And during the OPDES, reverse events are also implemented in a LIFO order (the first reverse event is used to remove the side effects produced by the last forward event). 
Therefore we only need one stack for all event handlers.

To let our stack hold variables of arbitrary types, one way in C++ is that for each variable we store a pointer of \texttt{void *} type pointing to the copy of the variable, and during the restoration, the referenced variable should be casted to the proper type.
However, when a great number of variables are stored, the memories possessing their copies may be scattered, and for each variable an additional size information is needed in the heap memory and is usually attached to the memory containing the variable, which can bring significant time and space overhead.
To amortize this overhead, we could use a memory pool that allocates memories to stored variables. 

In our simulation programs, we observed that most stored variables are of basic types like integers and floating points.
To decrease the memory used to store those variables, we create several special stacks which are used to store variables of basic types. 


\subsection{Storing C++ objects}

For a variable with basic types or POD (plain old data) types, there are no side effects when copying it during state savings.
However, for a C++ object of a class type, its copy constructor and assignment operator are triggered when storing and restoring it.
Backstroke requires that those two functions should be defined ``correctly''.
Here we discuss the correctness of those two functions.

Given an object $obj$ of type $class \;S$ in the original program, if it is stored and restored in the forward and reverse programs, we have to make sure the state remains unchanged after running the reverse program as it before running the  forward program.
We store an object by calling its copy constructor to create a copy of it which is stored in our stack based storage. 
We don't use assignment operator here because the class of the object may not have a default constructor, in which case creating an object is not that straight forward.
When we restore that object, normally the object is already there, and we will assign the stored one to it using assignment operator. 
After the restoration, we delete the stored object that calls the destructor of it.
We should guarantee that after storing and restoring an object the simulation state is correctly rolled back.
Suppose we have an object $obj$ with type $T*$, then the code below shows how we store it:

\begin{lstlisting}
T* objCopy = new T(*obj);  // The copy constructor is called.
storageStack.push(objCopy);
\end{lstlisting}

The code below shows how we restore this object:

\begin{lstlisting}
T* objCopy = storageStack.top();
storageStack.pop();
*obj = *objCopy;  // The assignment operator is called.
delete objCopy;  // The destructor is called.
\end{lstlisting}

We require that there is no side effect in the copy constructor, the assignment  operator, and the destructor. 
For instance, each of them should not modify a global or static shared variable that belongs to the simulation state and could affect the simulation result. 
In addition, the copy constructor and assignment operator usually should make a deep copy of the object.
This means if this object has a pointer pointing to another chunk of memory that is managed by this object, this piece of memory should also be copied in those two functions, and it needs to be released in the destructor. 
A good example is \texttt{std::vector}, whose copy constructor and assignment operator both make a deep copy of the array inside.


\subsection{Handling polymorphism}
Polymorphism is possibly the most important Object-Oriented (OO) feature in C++ and other OO languages. 
%\TODO{need a definition of polymorphism}
In C++, an object with a pointer or reference type $T$ may have a concrete type $S$ which is a subclass of $T$.
However, at compile time, it is impossible to identify the real type of such an object.
As a result, calling the copy constructor of the type $T$ will not do the whole copy of that object, but only a part of it that belongs to the type $T$ only.
This will lead to the error in state savings.
A virtual copy constructor helps, but it is not permitted in C++.
The same situation happens on assignment operator, which also cannot be virtual.
Among the three functions, only the destructor can be declared as a virtual function.

To get rid of this restrict of C++ language, we can define our own virtual versions of the copy constructor and assignment operator. 
Specifically, we require that the class $T$ should provide two special interfaces as virtual functions: a $clone$ function that returns a copy of the object, and a $assign$ function that assigns itself to another object.


\begin{lstlisting}
class T {
public:
    virtual T* __clone__();
    virtual void __assign__(T* obj);
    virtual ~T();
};

class S : public T {
public:
    virtual T* __clone__()
    { return new S(*this); }
    virtual void __assign__(T* obj)
    {  *this = *(dynamic_cast<S*>(obj)); }
    virtual ~S();
};
\end{lstlisting}

\begin{lstlisting}
T* objCopy = obj->__clone__();  // The virtual copy function is called.
storageStack.push(objCopy);
\end{lstlisting}

\begin{lstlisting}
T* objCopy = storageStack.top();
storageStack.pop();
obj->__assign__(objCopy);  // The virtual assignment function is called.
delete objCopy;  // The virtual destructor is called.
\end{lstlisting}




\section{Handling function calls}

In this section, we describe various techniques for extending our program inversionframework to handle programs with function calls. 
In addition, we discuss the pros and cons of these techniques.

\subsection{Inlining}
A nave technique of handling multiple function programs is to inline all the functions in the program such that program is effectively converted to a single function.The existing technique can then be applied to such a program to generate the corresponding forward program $P^+$ and inverse program $P^-$. 
However, there are two major drawbacks of this approach: 1) In programs with recursive calls, inlining can lead to a infinitely large program and thus, recursion needs to be handled in a special manner, 2) Even without recursion, inlining causes a large increase in the program size. 
Consequently, the corresponding VSG generated for this program is a verylarge graph. 
Performing the lowest-cost search on this graph for generating the RG then becomes practically infeasible. 
However, inlining does ensure that the most optimal solution compared to the other techniques described later.
\subsection{State Saving}
Another naive but practical approach is to save the incoming state before any function call. 
As a result, when generating the program inverse, it is possible to ignore the function call completely and restore the state saved before the call to the function. Although this method ensures correct programs and allows the application ofthe existing techniques, it negates the very purpose of generating inverse programs.By forcing a state save before every function call, this approach incurs alarge cost.

\subsection{Unique VSG and unique RG for each method}
In this approach, we generate a unique VSG for each non-virtual method. 
Further, considering the input parameters as the input state $I$ and the return value of the method as the output state $O$, we generate a RG for this method. 
Consider a method $foo()$. 
In the VSG of the callee of $foo()$, there exists a node which represents thecall to $foo()$. 
This node is connected to the return value via an incoming edge and has outgoing edges to the parameters of the called method. 
Using this approach, we add a new node in the VSG of the callee representing a call to $foo()^-$. 
The parameters of $foo()$ are connected to $foo()^-$ via edges incoming into $foo()^-$ while $foo()^-$ is connected via an outgoing edge to the return value of $foo()$. 
Further, the cost associated with these edges is as determined by the RG generated for $foo()$. 
This method allows a scalable approach to generating forward and reverse programs. 
Although the final program generated by inlining carries a lower costthan by this approach, it is possible to use this approach in a practical setting.However, this approach suffers from one major drawback. 
Suppose that a parameter of $foo()$ is overwritten just before the call to $foo()$.
The value of the parameter needs to be stored in the forward program $P^+$ since it is can not be recovered by computational methods. 
However, if the reverse program use $foo()^-$, it redundantly recovers the parameter value at the call to $foo()$ only for the recovered parameter value to be replaced by the stored value. 
This lack of flexibility in choosing the recovery mechanism for each parameter individually and being forced to use the same mechanism for all parameters leads to a sub-optimal solution.
\subsection{Unique VSG and multiple RG for each method}
This approach resolves the drawback in the previous approach. 
We still generate a unique VSG for each method. 
However, a separate RG is generated for each parameter with the parameter as the input state I and the return value of the method as the output state $O$. 
The separate RGs are then used to generate multiple inverse versions of the original method, with each inverse version generating a single parameter as the return value. 
Consider a method $foo()$ with parameters $a$ and $b$ and return variable $c$. Using this approach, we generate the inverse versions $foo()A^-$ and $foo()B^-$. In the VSG of the callee to $foo()$, there are edges from $c$ to $foo()$,from $foo()$ to $a$ and from $foo()$ to $b$. 
We additionally add edges from $a$ to $foo()A^-$, $b$ to $foo()B^-$, $foo()A^-$ to $c$ and $foo()B^-$ to $c$. 
This allows us to make independent decisions when generating the reverse code for each parameter of $foo()$.



\section{Handling virtual functions}



\section{Handling STL containers}

\newcommand{\stlvector}{\texttt{std::vector}\xspace}
\newcommand{\stldeque}{\texttt{std::deque}\xspace}
\newcommand{\stlqueue}{\texttt{std::queue}\xspace}
\newcommand{\pushback}{\texttt{push\_back()}\xspace}
\newcommand{\popback}{\texttt{pop\_back()}\xspace}
\newcommand{\push}{\texttt{push()}\xspace}
\newcommand{\pop}{\texttt{pop()}\xspace}

C++ STL containers are commonly used in the real code, including the simulation programs.
From high level, if we take an STL container instance as a state variable, and if it is modified in the event function, we have to use state saving technique that stores and restores this container.
From the respect of safety, if the type of the container elements has a proper copy constructor and a copy assignment operator,  the copy constructor and copy assignment operator of STL container will call it repeatedly on each element to store and restore the whole container;
from the respect of performance, this method suffers from bad efficiency: even if only one element in the container is modified, we have to copy the whole container during state saving.
Due to the restrictions of our method, we cannot handle the C++ STL container from low level (mass aliasing exists in STL container). 
In this section, we propose a method to ``undo'' some modifications on STL containers better than state saving, with the help of high level information. 


As a basic example, let take a look at the most commonly used STL container: \stlvector. 
A \stlvector works like an array, except its size can by adjusted dynamically at runtime.
It has an interface named \pushback which adds an element to the end of the vector, and hence the size of it is incremented by one. 
If a vector as a state variable is changed in this way, it is apparent that we can call its another interface \popback to undo this modification -  a much more efficient way than state saving. 
Now let's consider how to undo a \popback. 
Note that since the last element of the vector is popped and destroyed, we cannot call 
\pushback to undo the pop unless we have a copy of the popped element.
Here we come back to our forward-then-reverse function call approach.
In the forward function of \popback, we make a state saving on the being popped element before popping it, and in the reverse function of \popback, we can just restore the saved element and then call \pushback to add it back to the vector.
The forward and reverse functions of \pushback and \popback are generated by hand but can be recognized by Backstroke. They are shown below:

\begin{lstlisting}
template <class T>
inline void bs_vector_push_back_forward(std::vector<T>& v, const T& t) {
    v.push_back(t);
}
template <class T>
inline void bs_vector_push_back_reverse(std::vector<T>& v) {
    v.pop_back();
}
template <class T>
inline void bs_vector_pop_back_forward(std::vector<T>& v) {
    __store__(v.back());
    v.pop_back();
}
template <class T>
inline void bs_vector_pop_back_reverse(std::vector<T>& v) {
    T t;
    __restore__(t);
    v.push_back(t);
}
\end{lstlisting}

Now let's consider \stlqueue, which is  a container adapter that gives the programmer the functionality of a queue - specifically, a FIFO (first-in, first-out) data structure.
It has two interfaces that modify the internal data: \push and \pop.
\push pushes an object to the end of the queue and \pop pops the object at the front of the queue.
However, there is neither an interface that could push an object to the front, nor an interface that could pop an object from the end.
Hence it is impossible to write ``cheap''  reverse functions for \push and \pop.
But an \stlqueue actually contains a real container inside, and the default one is a \stldeque, which has more interfaces that we need.
Therefore, we can simply change the \stlqueue into a \stldeque then use the forward/reverse functions of it. 

The table below shows all STL containers with their interfaces which Backstroke would generate better forward/reverse functions that storing the whole container.

\begin{center}
    \begin{tabular}{ | p {4cm} | p {10cm} | }
    \hline
    STL container \& interface & how 
    \\ \hline
	vector::push\_back & do it
    \\ \hline
    vector::pop\_back & do it
    \\ \hline
    vector::insert & do it
    \\ \hline
    vector::erase & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline
    deque::push\_back & do it
    \\ \hline

    \end{tabular}
\end{center}

\bibliography{biblib2,more2,library2,biblib,more,library}

\end{document}
