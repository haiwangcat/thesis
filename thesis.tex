\documentclass[12pt]{gatech-thesis}
\usepackage{amsmath,amssymb,verbatim,latexsym,float,epsfig,enumerate,array}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{caption}
\DeclareCaptionType{copyrightbox}
\usepackage{subcaption}
\usepackage{multicol}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
\usepackage[ruled,vlined, boxed]{algorithm2e}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
\title{Automated synthesis for program inversion} %% If you want to specify a linebreak
                               %% in the thesis title, you MUST use
                               %% \protect\\ instead of \\, as \\ is a
                               %% fragile command that \MakeUpperCase
                               %% will break!
\author{Cong Hou}
\department{School of Computer Science}

%% Can have up to six readers, plus principaladvisor and
%% committeechair. All have the form
%%
%%  \reader{Name}[Department][Institution]
%%
%% The second and third arguments are optional, but if you wish to
%% supply the third, you must supply the second. Department defaults
%% to the department defined above and Institution defaults to Georgia
%% Institute of Technology.

\principaladvisor{Professor Richard Vuduc}
\committeechair{Professor Ignatius Arrogant}
\firstreader{Professor General Reference}[School of Mathematics]
\secondreader{Professor Ivory Insular}[Department of Computer Science and Operations Research][North Dakota State University]
\thirdreader{Professor Earl Grey}
\fourthreader{Professor John Smith}
\fifthreader{Professor Jane Doe}[Another Department With a Long Name][Another Institution]
%\setcounter{secnumdepth}{2}
\degree{Doctor of Philosophy}

%% Set \listmajortrue below, then uncomment and set this for
%% interdisciplinary PhD programs so that the title page says
%% ``[degree] in [major]'' and puts the department at the bottom of
%% the page, rather than saying ``[degree] in the [department]''

%% \major{Algorithms, Combinatorics, and Optimization} 

\copyrightyear{2013}
\submitdate{April 2013} % Must be the month and year of graduation,
                         % not thesis approval! As of 2010, this means
                         % this text must be May, August, or December
                         % followed by the year.

%% The date the last committee member signs the thesis form. Printed
%% on the approval page.
\approveddate{1 March 2013}

\bibfiles{example-thesis}

%% The following are the defaults
%%    \titlepagetrue
 \signaturepagefalse
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
%%    \bibpagetrue
\thesisproposalfalse
%%    \strictmarginstrue
%%    \dissertationfalse
%%    \listmajorfalse
%%    \multivolumefalse



\newcommand{\Drawgraph}[3]{
	\begin{figure}[H]
	\centering
	\begin{tikzpicture} [auto, >=stealth', scale=1]

	  \tikzstyle{array}=[rectangle, thick, minimum size=7mm, draw=black!75,font=\sffamily\small]
	  \tikzstyle{CFG}=[rectangle, thick, minimum size=7mm, draw=black!75,font=\sffamily\scriptsize,inner sep=0pt]
	  \tikzstyle{nothing}=[rectangle, thick, minimum size=7mm, draw=none]
	  \tikzstyle{scalar}=[circle, thick, minimum size=7mm, draw=black!75,font=\sffamily\tiny]
	  \tikzstyle{op}=[circle, , draw=black!75, fill=gray!25, minimum size=4mm, inner sep=0pt, font=\sffamily\tiny]
	   \tikzstyle{available}=[very thick]
	    \tikzstyle{reverse}=[dashed]
	    \tikzstyle{forward}=[densely dotted]
	  
 	 \tikzstyle{lbl}=[font=\sffamily]
  
	 #1
	
	\end{tikzpicture}
	\caption{#2}
	\label{#3}
	\end{figure}
}

\newcommand{\TODO}[1]{{\color{red}\textbf{#1}}}

\newcommand{\Code}[2]{
\begin{center}
    \begin{tabular}{ | p {6cm} | p {6cm} | }
    \hline
    Original Program & SSA Form \\ \hline
	$ \begin{aligned} 
	#1
	\end{aligned}  $
&
	$ \begin{aligned} 
	#2
	\end{aligned}  $
    \\ \hline
    \end{tabular}
\end{center}
}



\begin{document}
\bibliographystyle{gatech-thesis}

%%
\begin{preliminary}



% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.
\end{preliminary}
%%
\chapter{Introduction}


\newcommand{\naive}{na\"ive\xspace}
\newcommand{\Program}{\ensuremath{P}\xspace}
\newcommand{\Forward}{\ensuremath{\Program^{+}}\xspace}
\newcommand{\Inverse}{\ensuremath{\Program^{-}}\xspace}
\newcommand{\Input}{\ensuremath{I}\xspace}
\newcommand{\Output}{\ensuremath{O}\xspace}
\newcommand{\ExtraOuts}{\ensuremath{S}\xspace}
\newcommand{\OutsS}{\ensuremath{\Outs+\ExtraOuts}\xspace}
\newcommand{\Var}{\ensuremath{v}\xspace}
\newcommand{\Vars}{\ensuremath{V}\xspace}
\newcommand{\Exec}[4]{\ensuremath{{#1}\{{#2}={#3}\} \rightarrow \{{#2}={#4}\}}\xspace}


%\newcommand{\vmu}{\ensuremath{v_{in}^I}\xspace}
%\newcommand{\vinit}{\ensuremath{v_{in}}\xspace}
%\newcommand{\veta}{\ensuremath{v_{\eta}}\xspace}
%\newcommand{\vfinal}{\ensuremath{v_{out}}\xspace}
%\newcommand{\vmup}{\ensuremath{v_{\mu}'}\xspace}
%\newcommand{\viter}{\ensuremath{v_{out}^I}\xspace}
%\newcommand{\viterp}{\ensuremath{v_{iter}'}\xspace}
%\newcommand{\mufunc}{\ensuremath{v_{in}^I=\mu(v_{in},v_{out}^I)}\xspace}
%\newcommand{\etafunc}{\ensuremath{v_{out}=\eta(v_{in}^I)}\xspace}
\newcommand{\varmbox}[2]{\ensuremath{{#1}_{\tiny\mbox{#2}}}}
\newcommand{\vmu}{\ensuremath{\varmbox{v}{in}^I}\xspace}
\newcommand{\vinit}{\ensuremath{\varmbox{v}{in}}\xspace}
\newcommand{\veta}{\ensuremath{\varmbox{v}{\eta}}\xspace}
\newcommand{\vfinal}{\ensuremath{\varmbox{v}{out}}\xspace}
\newcommand{\vmup}{\ensuremath{\varmbox{v}{\mu}'}\xspace}
\newcommand{\viter}{\ensuremath{\varmbox{v}{out}^I}\xspace}
\newcommand{\viterp}{\ensuremath{\varmbox{v}{iter}'}\xspace}
\newcommand{\mufunc}{\ensuremath{\varmbox{v}{in}^I=\mu(v_{in},v_{out}^I)}\xspace}
\newcommand{\etafunc}{\ensuremath{\varmbox{v}{out}=\eta(v_{in}^I)}\xspace}
\newcommand{\Loop}{\ensuremath{L}\xspace}


We consider the problem of synthesizing \emph{program inverses}.
That is, given a program \Program with input state \Input and output state \Output, its \emph{inverse} or \emph{reverse program}, \Inverse, produces \Input given \Output.
Our primary motivation comes from optimistic parallel discrete event simulation (OPDES).
There, a simulator must process events while respecting logical temporal event-ordering constraints;
to extract parallelism, an OPDES simulator may speculatively execute events and only \emph{rollback} execution when event-ordering violations occur~\cite{Jefferson1985}.
In this context, the ability to perform rollback by running time- and space-efficient \Program and \Inverse, rather than saving and restoring large amounts of state, can make OPDES more practical.
%
Synthesizing inverses also appears in numerous other software engineering contexts, such as debugging~\cite{Biswas1999}, synthesizing ``undo'' code, or even generating decompressors automatically given only lossless compression code~\cite{Srivastava2011}.
The challenge in any of these contexts is that constructing program inverses manually is a tedious, time-consuming, and error-prone task.

%Running the original \Program followed immediately by \Inverse is equivalent to a no-op.
The program \Program will generally contain non-invertible statements, such as a destructive assignment.
However, in these cases it may still be possible to create an inverse.
In particular, we may create an instrumented version of \Program, called the \emph{forward program}, \Forward, with semantics-preserving changes so that it becomes possible to construct \Inverse from \Forward.
We then replace executions of \Program with \Forward.
For instance, suppose \Program overwrites a variable \Var.
We may construct \Forward so that it saves the value of \Var prior to overwriting it.
Then, \Inverse need only restore the saved value to recover \Var's prior value.
In this case, \Forward produces extra outputs, which we denote by \ExtraOuts.
Indeed, even if \Program is theoretically reversible without requiring extra output, we may nevertheless need to generate \ExtraOuts due to fundamental technical limits on program analysis.

In this thesis, we will introduce our system that generates \Forward and \Inverse for a given \Program. 
Specifically, we categorize \Program into four types: \Program with only scalars and without loops; \Program with only scalars and with loops; \Program with arrays and without loops; \Program with arrays and with loops.
We will first introduce how we handle the first three cases using a bunch of compilation techniques.
Handing programs with arrays in loops is exactly the proposed work, being introduced in the last chapter.


\section{Related work}
\label{sec:related-work}
Most of the work on inverting arbitrary (non-injective) imperative programs has focused an incremental approach: the imperative program is essentially executed in reverse, with each modifying operation in the original execution being undone individually.
For example, if statements $s_{1} s_{2} \dots s_{n}$ are executed in the forward directions, the reverse function executes statements $s_{n}^{-1} \dots s_{2}^{-1} s_{1}^{-1}$.
The incremental approach cannot handle unstructured control flows and is difficult to apply with early returns from functions; the approach presented in this thesis suffers from neither of these shortcomings.  
Furthermore, the incremental inversion restores the initial state by restoring every intermediate program state between the final state and the initial state, even though these states are not needed

Among the incremental inversion approaches, syntax-directed approaches apply only statement-level analysis. 
If an assignment statement is lossless, its inverse is used: for example, the inverse of an integer increment is an integer decrement.
Otherwise, the variable modified in the assignment has to be saved.
An early example of syntax-directed incremental inversion is Brigg's Pascal inverter \cite{Briggs1987}. 
This approach was later extended to C and applied both to optimistic discrete event simulation \cite{Carothers1999} and reversible debugging \cite{Biswas1999}.

Akgul and Mooney introduced a more sophisticated incremental inversion algorithm that uses def-use analysis to invert some assignment statements that are not lossless \cite{Akgul2004a}; we refer to this approach as regenerative incremental inversion.
In order to reverse a lossy assignment to the variable $a$, such as $a \leftarrow 0$, the regenerative algorithm looks for ways to recompute the previous value of $a$. 
One technique to obtain the previous value of $a$ is to re-execute its definition; another technique is to examine all the uses of $a$ and see if its value can be retrieved from any of its uses. 
These two techniques are applied recursively whenever a modifying operation is to be reversed; if they fail to produce a result, the overwritten variable is saved during forward execution.
Our approach takes advantage of all the def-use relationships utilized by regenerative inversion, without suffering from the drawbacks of incremental inversion. 
In addition to def-use information, our approach  also derives equality relationships between variables from the outcome of branching statements that test for equality or inequality.

A related line of work is inverting programs that are injective, without using any state saving.
Most such work focuses on inverting functional programs \cite{Abramov2002a,Gluck2005,Kawabe2005}.
Approaches to inverting imperative programs include translation to a logic language \cite{Ross1997} and template-based generation of inverse candidates using symbolic execution \cite{Srivastava2011}.




\chapter{Synthesis for programs with only scalars}


%============================================================
\section{Handling loop-free programs}
\label{sec:scalar-loop-free}
\input{scalar-loop-free}

%============================================================
\section{Handling programs with loops}
\label{sec:scalar-loops}
\input{scalar-loops}


%============================================================
%============================================================


\chapter{Synthesis for programs with arrays}
\label{chapter:arrays-loop-free}

\TODO{To be revised}
In this chapter, we extend the previous method to handling arrays in both loop-free programs and those with loops.

There are two motivations of it: in optimistic  parallel discrete event simulation, if we can avoid performing state saving on an array that is modified in a loop and can be retrieved by another loop in the reverse program, the performance will be improved; 
we also consider to generate the inverse for injective programs, such as lossless compression and encryption,  when state saving is not allowed (because the program is already injective or reversible, it is not necessary to generate a forward program). 

Arrays are a particularly important case: naively saving the entire array too frequently will be very expensive in space and time. Our method permits saving of just the elements that have changed, and better yet, can often use cheaper computation in place of state saving.

To build the VSG for arrays, we apply a modified Array SSA based on \cite{}, and define several special VSG nodes for arrays. 
To represent the equality between two arrays, we employ the array subregion as the constraint. 
During the search those subregions will be calculated to guarantee that all array elements will be retrieved. We also develop a demand-driven method to retrieve array elements from a loop. 

%============================================================
\section{Handling loop-free programs with arrays}
\label{sec:array-loop-free}
\input{arrays}

%============================================================
\section{Handling programs with loops and arrays}
\label{sec:array-loops}
\input{array-loops}



\begin{comment}
\chapter{Synthesis for programs with arrays}

Usually data-flow analysis for arrays is more difficult than that for scalars. 
Unlike an scalar, each element in an array does not have a static unique name, but is represented as an array name and an index. 
Two variables as indices of the same array with the same value bring aliases to the indexed element, and this aliasing is difficult to be detected at compile time. 
Fortunately, with the help of the Array SSA \cite{Rus} proposed for programs with arrays, we could build the VSG for such programs and develop a searching rule in order to generate a valid RG. 
The Array SSA tries to match definitions and uses of partial array regions, and we will also employ such array regions in our VSG as another condition of equalities between arrays (the condition we previous used is  control flow path set). 
As a result, we can still reuse the whole framework we previously proposed  to handle programs with arrays.

In this chapter, we first introduce the Array SSA, then describe how we make use of it and add array nodes and edges in the VSG. Then we still divide the programs with arrays into two categories: programs without and with loops. 
We will show how we handle loop-free programs, and propose a method to deal with more challenging programs with loops.
%In this chapter we will use an Array SSA proposed in [] in which concrete array regions are employed.
%Based on this Array SSA, we could build the VSG with special array nodes inside.
%By modifying the searching rules on the VSG, we will see that we could handle programs with arrays using the same framework as before. 


\section{Array SSA}

In the original SSA, when an element of an array is modified, the array itself is assigned with a new version, and the whole array is killed by that definition. 
In Array SSA, the definition of an array element only kills the previous definition of that element represented by a subregion but not the whole array. 
This new SSA form representation can accurately represent the use-def relations between array regions. 

There are three special $\phi$ functions defined for arrays: a $\delta$ function accounts for a partial kill after an element of the array is defined; a $\mu$ function is defined at the beginning of the loop header just as it for scalars; a $\eta$ function defined at the exit of a loop as the output of the loop. 
%a $\gamma$ function defined at the join node in CFG joining definitions of arrays from different control flow paths.

%there is one special $\phi$ function (called a $\delta$ function) added just after the definition, from which a partial kill is represented. For example:


\subsection{$\delta$ function}
 
 When an array element  is modified, except renaming the array's name (assigning it with a new version), a $\delta$ function is defined just after the definition to that element. 
The $\delta$ function summarizes the data-flows of all elements in the array and builds accurate def-use relations with the help of array regions.
 The syntax of a $\delta$ function is shown below:
 $$[a_n, @a_n] = \delta(a_0, [a_1, @a_1^n], [a_2, @a_2^n],  ... , [a_m, @a_m^n] )$$
 $$where \; @a_n=\bigcup_{k=1}^m@a_k^n \; and \; @a_i^n \cap a_j^n = \emptyset, \forall 1\le i,j \le m, i \ne j$$

In the equation above, $@a_k^n$ represents the array region in which the definitions of all elements of $a_k$ are not killed before the definition of $a_n$.
 The code below shows two $\delta$ functions after the definitions of two array elements respectively.
 \begin{flalign*} 
& int \; a_0[N], i_0, j_0; \\
& a_1[i_0] = ...;\\
& [a_2, @a_2] = \delta (a_0, [a_1, @a_1^2]); \\
& a_3[j_0] = ...;\\
& [a_4, @a_4] = \delta (a_0, [a_2, @a_2^4], [a_3, @a_3^4]); 
\end{flalign*} 
where $@a_1^2=@a_2=\{i_0\}, @a_2^4=\{i_0\}-\{j_0\}, @a_3^4=\{j_0\}, @a_4=\{i_0\}\cup\{j_0\}$. Note each array region is represented by a set including symbols (constants or SSA names). We will discuss the representation of array regions later.

In this example, after the definition in the second line, there is a $\delta$ function that defines a new version of the array $a$ as $a_2$. 
%In addition, each argument of the $\delta$ function is a definition of the array with a region. 
The argument $ [a_1, @a_1^2]$ of the $\delta$ function means that all elements of $a_2$ in the region $@a_1^2=\{i_0\}$ are defined by $a_1$. 
The first argument $a_0$ has the definitions of the elements in the region not belonging to other arguments.
 %Each argument except the first one is a tuple $[a_n, @a_n]$ including a definition of the array and the corresponding array region. 
% All elements in this region The $\delta$ function summarizes the reaching definitions of all elements: after one of them is redefined: 

%In this example, after the definition in the second line, there is a $\delta$ function that defines a new version of the array $a$ as $a_2$. In $a_2$, only the value indexed by $i$ equals that in $a_1$, and any other value equals the corresponding one in $a_0$. This means only the $i$th value of $a_0$ is killed by the definition of $a_1$.


\subsection{$\mu$ and $\eta$ functions}

Preiously when we handle loops with scalars, we introduced a $\mu$ function for each variable modified in the loop. 
The arguments in the $\mu$ function come from outside and inside of the loop, and each definition of the $\mu$ function kills all previous definitions. 
In Array SSA,  for each array modified in the loop we also define a $\mu$ function in the loop header receiving definitions from outside and inside of the loop. However, like a $\delta$ function, a $\mu$ function only kills the definition in some regions of the array, and the region is related to the loop index. Let $i$ be the loop index, then all regions in a $\mu$ function are functions of $i$. The syntax of a $\mu$ function is defined by the following equation.
 $$[a_n, @a_n] = \mu(a_0, (i=1,p), [a_1, @a_1^n(i)], [a_2, @a_2^n(i)],  ... , [a_m, @a_m^n(i)] )$$

In this equation the sets $@a_k^n(i)$ are functions of the loop index $i$ and they represent the sets of memory locations defines in some iterations $j<i$ by definition $a_k$ and not killed before reaching the beginning of iteration $i$. 
For any array element defined by $a_k$ in some iteration $j < i$, in order to reach iteration $i$, it must not be killed by other definitions to the same element. 
There are two kinds of definitions that will kill it: definitions ($Kill_s$) that will kill it within the same iteration $j$ and definitions ($Kill_a$) that will kill it at iterations from $j +1$ to $i-1$.
The definition of $@a_k^n(i)$ is shown below.
 $$@a_k^n(i)=\bigcup_{j=1}^{i-1}\left[@a_k(j)-\left(Kill_s(j) \cup \bigcup_{l=j+1}^{i-1}Kill_a(l)\right)\right]$$
 $$ where \; Kill_s = \bigcup_{h=k+1}^m @a_h, \; and \; Kill_a=\bigcup_{h=1}^m@a_h$$

%\section{Arrays in the VSG}

\section{Handling arrays in loop-free programs}

\subsection{Array region representation}

%Unless explicitly indicated, when we retrieve an array, we are retrieving all elements of it. 
Given a C/C++ style array $a[N]$ with length $N$, then all elements can be represented by an interval $[0,N-1]$, or $[0:N-1]$. 
We will use this interval as a set that includes all integers in that interval, and hence an array region becomes an integer set.
%Here $[0,N)$ represents a set containing all integers in this interval. 
%We will use this representation through this report. 
$[0:N-1]$ represents the \emph{universal set} of all regions of the array $a$, denoted by $\mathcal{A}(a)$. 
%Instead of using $@a$, we use $R$ to represent a region in an array. 

Now we define some basic array regions that are commonly used later:
%Among all its subsets, there are some special ones: 

\begin{itemize}
\item The region containing only one integer is represented by $\{i\}$, where $i$ is a constant or an SSA name. 
\item The complementary set $\overline{\{i\}} = \mathcal{A}(a)-\{i\}$ represents all other elements except the $i$th one. 

\item The triplet representation $[a:b:c]$ represents all integers between $a$ and $c$ with stride $b$. If $b=1$, it can be simplified as $[a:c]$. 

\end{itemize}

All set operations can be performed on array regions. Actually, we will rely on those operations to determine if all elements of an array are retrieved during the search on the VSG. 
In order to build a more accurate RG, we need to simplify an expression with set operations.
The process of simplification  depends on the properties of set operations, including identity laws, domination laws, idempotent laws, commutative law, associative laws, distributive laws, etc..
In addition, since an array region set may contain SSA names. 
The knowledge of the relations between them can also help to simplify an set expression.
For example, $\{i\} \cup \{j\} = \{i\} =\{j\}$ if $i=j$, and $\{i\} \cap \{j\} = \emptyset$ if $i\ne j$.
%For example, $\{i\} \cup \overline{\{i\}} = [0,N)=U$. 

%For induction variables in a loop (assume it has depth one), we use two regions for arrays indexed by an induction variable: one for the local iteration, and one for a summary for the scope outside of the loop. 

\subsection{Arrays in the VSG}

For each definition of an array we add a special node in the VSG as its representation and we call it an \emph{array node}. 
Any element of an array is a scalar and is still denoted by a normal value node. 
%Remember that every edge in a VSG is attached with the path information.
For any edge between two array nodes $a_x$ and $a_y$, an array region $R$ is attached to it, showing that $a_x$ and $a_y$ have the same values for all elements in the region $R$.
For each access of an array, e.g. $a_x[i]$, we also add a region $\{i\}$ on the edge between VSG nodes representing $a_x$ and $a_x[i]$.

We will build the VSG for arrays based on the Array SSA with some modifications. 
%Let's see first how we build arrays nodes for a $\delta$ function. 
In Array SSA, a $\delta$ function is always defined under a definition to an element of it, and those two definitions create two names of the same array. 
To reduce the number of array nodes in VSG, we combine those two names into one.
%, and connect this array node to other array nodes and the definition of its element. 
For example, for the following code:
\begin{flalign*} 
& int \; a_0[N]; \\
& a_1[i] = ...;\\
& [a_2, @a_2] = \delta (a_0, [a_1, @a_1^2]); \\
& a_3[j] = ...;\\
& [a_4, @a_4] = \delta (a_0, [a_2, @a_2^4], [a_3, @a_3^4]); 
\end{flalign*} 
We will transform it into the code below:
\begin{flalign*} 
& int \; a_0[N]; \\
& a_1[i] = ...;\\
& a_1 = \delta ([a_0, \overline{\{i\}}], [a_1, \{i\}]);  \\
& a_2[j] = ...;\\
& a_2 = \delta ([a_1, \overline{\{j\}}], [a_2, \{j\}]); 
\end{flalign*} 

After the definition of $a_1$, instead of creating a new name $a_2$, we maintain a correct equality relations between $a_1$ and $a_0$, and also $a_1$ and $a_1[i]$. 
In the VSG, we  add an edge with region $\overline{\{i\}}$ between nodes of $a_0$ and $a_1$, and  add another edge with region $\{i\}$ between node of $a_1$ and $a_1[i]$. 
With the help of this region information, we can make a full kill to $a_0$ when $a_1$ is defined (that is, $a_0$ will never be used after the definition of $a_1$). 
Similarly, when $a_2[j]$ is defined, we create another $\delta$ function that shows the equality between $a_1$ and $a_2$ in the region $\overline{\{j\}}$, and $a_2$ and $a_2[j]$ in the region $\{j\}$.
In this way, a $\delta$ function always has two arguments: one is the newly defined array with the region $\{i\}$ where $i$ is the index of the defined element; one is the reaching definition of the array before defining the element, with the region $\overline{\{i\}}$.
%As a consequence, all later use of $a_2$ will be replaced by $a_1$.
The created nodes and edges in the VSG are shown below:


\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=above:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \path
    (a1) edge node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge node  [lbl, swap] {$\overline{\{j\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    ;
}{The VSG edges with array regions between array nodes.}{fig:array-vsg}

In the VSG, each array node is represented by a square node, being differentiated from circle nodes for scalars. 
For each element access of an array $a_x[i]$, the corresponding value node will be connected to the array node for $a_x$ with region $\{i\}$.
%The region informations are attached to edges incident to array nodes.

In Array SSA,  $\phi$ functions\footnote{In the original paper of Array SSA, it is called a $\gamma$ function.} defined for arrays at a join node in the CFG has the same meaning as those for scalars.
%there is also a for an array if it has different definitions from incoming edges. In this case, we take it in the same way as scalars. 
Therefore, in the VSG we build a $\phi$ array node representing the array defined by the $\phi$ function and connect it to all of its arguments with correct control flow path set and  the full array region. 






%Note that an array can only be modified through each of its elements.

\subsection{Search the VSG to retrieve an array}

The searching rules for arrays on the VSG is similar to that for scalars, except we have to take the array region into account.
In the VSG, every edge incident to an array node has both control path information and array region information. 
During the search, once such an edge is selected, the search begins to have an addition goal to retrieve all elements of the corresponding array in the region on that edge.
Only when the search reaches available array nodes or scalar value nodes can the region goal added to the search be removed.
At the beginning of the search, if an array node is the target node, then the search needs to retrieve all elements of the array.
%The special searching rules for arrays:

During the search on the VSG, it is possible that an edge is selected several times in searching for more than one values with different control flow paths and array regions. 
Therefore, in the RG, the control flow path and array region sets are paired as $[P,R]$, where $P$ and $R$ stand for the path set and array region set respectively.
 And it is possible that an edge $e$ has several such pairs $[P_0,R_0],[P_1,R_1],...,[P_n,R_n]$.
We require that $P_i \cap P_j = \emptyset, i\ne j$.
Then for the edge $e$ we can get those sets through $P_i(e)$ or $R_i(e), i = 0,...,n$.

As previously, the searching rule should guarantee the search result RG to meet several properties. 
For each edge $e$ in the RG, let $P(e)$ denote the control flow path set on it, and let $R(e)$ denote the array region set on it.
Those properties include:

\begin{enumerate}[I)]

\item For each target array node $n$ of the array $a$, and for each control flow path $p$,
	$$\bigcup_{\substack{\mathit{out} \in \text{OutEdges}(n)  \\  p \in P_i(out)}}R_i(\mathit{out}) \quad = \quad \mathcal{A}(a) $$ 
	\label{rg-property-1}

\item For each array node $n$ that is neither a target node nor an available node, and for each control flow path $p$,
	$$\bigcup_{\substack{\mathit{out} \in \text{OutEdges} (n) \\ p \in P_i(out)}} R_i(out) = \bigcup_{\substack{\mathit{in} \in \text{InEdges} (n)  \\  p \in P_j(in) }}R_j( \mathit{in} )$$ 

\item For each array node $n$, given any two outgoing edges $n\to s$ and $n\to t$, for each control flow path $p$, and $p \in P_i(n \to s)$, and $p \in P_j(n \to t)$, then $R_i(n\to s) \cap R_j(n\to t) = \emptyset$.

\item If $e$ is a route graph edge and its corresponding edge in the VSG is $e'$, then for each $R_i(e)$,
$R_i(e) \subseteq R(e')$.
	\label{rg-property-4}

%\item For each directed cycle with edges $e_1 \dots e_n$,  $\quad \bigcap_{i=1}^n{P(e_{i})} = \emptyset$
	%\label{rg-property-5}

\end{enumerate}




%1. For each edge incident to an array node, except the path information, there is also region information. 

%2.  If the search starts from an array node, it tried to retrieve all elements of that array. Then the region $[0,N)$ is attached to this search. This is very similar to the path information. 

%3. When the search reaches an element node from an array node, the region of the search can be removed. The search from node a to node b has the region information iff a or b is an array node.

%4. In the RG, for each array node, R(in)=R(out).


\subsection{State saving on an array and its elements}

In the VSG, to enable the capability of generating state saving statements in forward program, there is still a state saving node, which is connected to all scalar value nodes as well as array nodes. 
Each state saving edge connected to an array nodes of the array $a$ has the region   $\mathcal{A}(a)$.
When the search reaches a state saving edge incident to an array node, the array region on it will be calculated and updated according to the searching rule. 
As a result, it is possible that the final region on this state saving edge is unknown at compile time or is represented by an expression with several set operations.
To facilitate the code generation, we require that the region on a state saving edge must fall into two categories: a single element, or the full region. 
In the first case, we only need to store a single element of the array; in the second case, we will store the whole array by building a loop, and the size of the loop needs to be retrieved before storing the whole array. 


\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=below:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \node [scalar] (ss) at (9,0) {SS};
    \path
    (a1) edge node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge node  [lbl, swap] {$\overline{\{j\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    (a0) edge [bend left] (ss)
    (a1) edge [bend left] (ss)
    (a1i) edge (ss)
    (a2j) edge (ss)
    ;
}{A state saving node connecting all value nodes.}{fig:array-ss-vsg}

\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=below:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \node [scalar] (ss) at (9,0) {SS};
    \path
    (a1) edge [pre, red,ultra thick] node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge [pre, red,ultra thick] node  [lbl, swap] {$\overline{\{j\}} \cap \overline{\{i\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    (a0) edge [post, red,ultra thick,bend left] node  [lbl] {$\{i\}$} (ss)
    (a1) edge [post, red,ultra thick,bend left] node  [lbl,swap] {$\{j\}\cap \overline{\{i\}}$} (ss)
    (a1i) edge (ss)
    (a2j) edge (ss)
    ;
}{The search result is shown in bold red edges.}{fig:array-ss-rg}


For example, Figure~\ref{fig:array-ss-vsg} shows the VSG for the code we used before, and Figure~\ref{fig:array-ss-rg} is the search result RG shown as bold edges.
The array region on the edge from $a_1$ to $SS$ node is $\{j\}\cap \overline{\{i\}}$.
And $\{j\}\cap \overline{\{i\}} = \left\{ \begin{array}{rcl}  \{j\} & \mbox{if} & i \ne j \\ \emptyset & \mbox{else} \end{array}\right.$. 
If we cannot resolve the result of it at compile-time (that it, we don't know if $i$ and $j$ have the same value), we will use $\{j\}$ instead of $\{j\}\cap \overline{\{i\}} $ in the RG.
In the generated code, we always perform a state saving on $a_1[j]$.

\end{comment}

%\section{Handling arrays with loops}

%\section{Handling object access}

%\section{Handling linked data structure}

%This is what to be proposed in this proposal.


\section{Handling linked data structures}
\label{linked-data-structure}

After we solve the problems with arrays, we will try to extend this method to reversing programs with linked data structures. 
The basic idea is that linked data structure can be transformed into arrays:
in C/C++, a linked data structure is accessed by a pointer, and through this pointer we can get the data inside on each field. 
Then we can treat each field as an array, and the pointer as the index of it.
The code below gives an example of a data structure list, and in the loop the integer value of each node in the list is increased by one.

 \begin{flalign*} 
 & struct \;  Node \; \{ \\
 & \;\;\;\;  int \; val; \\
 & \;\;\;\; Node* \; next; \\
 & \}; \\
 & Node* \; head; \\
 & while\; (head \; != \; NULL)\;\{ \\
 & \;\;\;\; head->val++;\\
 &\;\;\;\; head = head->next;\\
 &\}
 \end{flalign*} 

In the code above, we can treat $val$ and $next$ as two arrays, then the loop will be transformed as the code below:
 \begin{flalign*} 
 & while\; (head \; != \; NULL)\;\{ \\
 & \;\;\;\; val[head]++;\\
 &\;\;\;\; head = next[head];\\
 &\}
 \end{flalign*} 
 
However, unlike the index of an arrays that may have regular access patterns, we usually don't have enough information of the pointers pointing to linked data structures. 
We will try to find out in which situation or under what conditions can we handle them.




\chapter{Synthesizing for C++ programs}

In prior chapters, we target programs in no specific languages, as our method is general and can be applied on any imperative language.
In this chapter, we introduce some techniques of handling some high-level constructs of C++ programs. 
We choose C++ for three reasons:
first, Backstroke is based on ROSE compiler, which is a C++ source-to-source compiler; 
second, C++ is an object-oriented (OO) language, and our discussion in this chapter can also be extended to other OO languages like Java;
third, initially Backstroke is used to generate reverse functions for OPDES events, and our  simulation engines (GTNets and ROSS) are writing in C/C++.

\section{Normalizing C++ programs}

Backstroke is a source-to-source translator, and is based on ROSE,  a C++ source-to-source compiler. 
The ROSE compiler parses the source code and transform it into an abstract syntax tree (AST) as the intermediate representation (IR).
However, no further IR like three address code is generated after that, making it difficult to build some constructs relying on low level IRs. 

Our method heavily depends on SSA form, and building it on source code is challenging, as the source may contain complex statements or expressions.
In addition, because the forward program is generated by instrumenting state saving and path recording statements in the original program, normalizing the program can make it easier to find those instrumentation locations in the program. 
%Therefore, before we generate the SSA form, we first normalize the AST into a regular one.

\subsection{Forcing a specific execution order of several expressions}
%1. If the execution order of two expressions are not defined, we force a specific order.

In C++, there is a concept called \emph{sequence point} that is used to indicate the execution order of several expressions. 
A sequence point defines any point in a computer program's execution at which it is guaranteed that all side effects of previous evaluations will have been performed, and no side effects from subsequent evaluations have yet been performed.
Normally, all expressions in a statement are executed before the next statement. 
But in one statement, several expressions with side effects may exist. 
In this case, the C++ standard  only guarentees the execution order for the following expressions:

\begin{itemize}

\item Comma expression: $expr1, expr2$. 
In this expression, $expr1$ must be executed before $expr2$, and the returned value of $expr2$ is used as the returned value of this comma expression.

\item Logical and/or operator: $expr1 \;\&\&\; expr2$ / $expr1\; ||\; expr2$. 
In this expression, $expr1$ must be executed first, and $expr2$ is  executed only if the returned value of $expr1$ is $true$/$false$ for logical and/or operations.
This is called \emph{short-circuit evaluation}.

\item Conditional operator: $expr1 \;?\; expr2 : expr3$. In this expression, $expr1$ will be executed first, and if the returned value is $true$, then $expr2$ is  executed; otherwise, $expr3$ is executed.

\end{itemize}



\section{State savings on C++ objects}

For a variable with basic types or POD (plain old data) types, there are no side effects when copying it during state savings.
However, for a C++ object of a class type, its copy constructor and assignment operator are triggered when storing and restoring it.
Backstroke requires that those two functions should be defined ``correctly''.
Here we discuss the correctness of those two functions.

Given an object $obj$ of type $class \;S$ in the original program, if in the forward program it is stored and in the reverse program it is restored, we have to make sure the state remains unchanged after running the reverse program as it before running the  forward program.

\section{Handling function calls}

\section{Handling virtual functions}

\section{Handling STL containers}

\newcommand{\stlvector}{\texttt{std::vector}\xspace}
\newcommand{\pushback}{\texttt{push_back()}\xspace}
\newcommand{\popback}{\texttt{pop_back()}\xspace}

C++ STL containers are commonly used in the real code, including the simulation programs.
From high level, if we take an STL container instance as a state variable, and if it is modified in the event function, we have to use state saving technique that stores and restores this container.
From the respect of safety, if the type of the container elements has a proper copy constructor and a copy assignment operator,  the copy constructor and copy assignment operator of STL container will call it repeatedly on each element to store and restore the whole container;
from the respect of performance, this method suffers from bad efficiency: even if only one element in the container is modified, we have to copy the whole container during state saving.
Due to the restrictions of our method, we cannot handle the C++ STL container from low level (mass aliasing exists in STL container). 
In this section, we propose a method to ``undo'' some modifications on STL containers better than state saving, with the help of high level information. 


As a basic example, let take a look at the most commonly used STL container: \stlvector. 
A \stlvector works like an array, except its size can by adjusted dynamically at runtime.
It has an interface named \texttt{push\_back()} which adds an element to the end of the vector, and hence the size of it is incremented by one. 
If a vector as a state variable is changed in this way, it is apparent that we can call its another interface \texttt{pop\_back()} to undo this modification -  a much more efficient way than state saving. 
Now let's consider how to undo a \texttt{pop\_back()}. 
Note that since the last element of the vector is popped and destroyed, we cannot call 
\texttt{push\_back()} to undo the pop unless we have a copy of the popped element.
Here we come back to our forward-then-reverse function call approach.
In the forward function of \texttt{pop\_back()}



\bibliography{biblib2,more2,library2,biblib,more,library}

\end{document}
