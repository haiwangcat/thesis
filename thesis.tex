\documentclass[12pt]{gatech-thesis}
\usepackage{amsmath,amssymb,verbatim,latexsym,float,epsfig,enumerate,array}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{caption}
\DeclareCaptionType{copyrightbox}
\usepackage{subcaption}
\usepackage{multicol}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
\usepackage[ruled,vlined, boxed]{algorithm2e}
\usepackage{color}
\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
\title{Automated synthesis for program inversion} %% If you want to specify a linebreak
                               %% in the thesis title, you MUST use
                               %% \protect\\ instead of \\, as \\ is a
                               %% fragile command that \MakeUpperCase
                               %% will break!
\author{Cong Hou}
\department{School of Computer Science}

%% Can have up to six readers, plus principaladvisor and
%% committeechair. All have the form
%%
%%  \reader{Name}[Department][Institution]
%%
%% The second and third arguments are optional, but if you wish to
%% supply the third, you must supply the second. Department defaults
%% to the department defined above and Institution defaults to Georgia
%% Institute of Technology.

\principaladvisor{Professor Richard Vuduc}[School of Computational Science and Engineering]
\firstreader{Professor Richard Fujimoto}[School of Computational Science and Engineering]
\secondreader{Professor Santosh Pande}[School of Computer Science]
\thirdreader{Doctor Daniel Quinlan}[Center for Applied Scientific Computing][Lawrence Livermore National Laboratory]
\fourthreader{Doctor David Jefferson}[Center for Applied Scientific Computing][Lawrence Livermore National Laboratory]
%\setcounter{secnumdepth}{2}
\degree{Doctor of Philosophy}

%% Set \listmajortrue below, then uncomment and set this for
%% interdisciplinary PhD programs so that the title page says
%% ``[degree] in [major]'' and puts the department at the bottom of
%% the page, rather than saying ``[degree] in the [department]''

%% \major{Algorithms, Combinatorics, and Optimization} 

\copyrightyear{2013}
\submitdate{May 2013} % Must be the month and year of graduation,
                         % not thesis approval! As of 2010, this means
                         % this text must be May, August, or December
                         % followed by the year.

%% The date the last committee member signs the thesis form. Printed
%% on the approval page.
\approveddate{1 May 2013}

\bibfiles{example-thesis}

%% The following are the defaults
%%    \titlepagetrue
 \signaturepagefalse
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
%%    \bibpagetrue
\thesisproposalfalse
%%    \strictmarginstrue
%%    \dissertationfalse
%%    \listmajorfalse
%%    \multivolumefalse



\newcommand{\Drawgraph}[3]{
	\begin{figure}[H]
	\centering
	\begin{tikzpicture} [auto, >=stealth', scale=1]

	  \tikzstyle{array}=[rectangle, thick, minimum size=7mm, draw=black!75,font=\sffamily\small]
	  \tikzstyle{CFG}=[rectangle, thick, minimum size=7mm, draw=black!75,font=\sffamily\scriptsize,inner sep=0pt]
	  \tikzstyle{nothing}=[rectangle, thick, minimum size=7mm, draw=none]
	  \tikzstyle{scalar}=[circle, thick, minimum size=7mm, draw=black!75,font=\sffamily\tiny]
	  \tikzstyle{op}=[circle, , draw=black!75, fill=gray!25, minimum size=4mm, inner sep=0pt, font=\sffamily\tiny]
	   \tikzstyle{available}=[very thick]
	    \tikzstyle{reverse}=[dashed]
	    \tikzstyle{forward}=[densely dotted]
	  
 	 \tikzstyle{lbl}=[font=\sffamily]
  
	 #1
	
	\end{tikzpicture}
	\caption{#2}
	\label{#3}
	\end{figure}
}

\newcommand{\TODO}[1]{{\color{red}\textbf{#1}}}

\newcommand{\Code}[2]{
\begin{center}
    \begin{tabular}{ | p {6cm} | p {6cm} | }
    \hline
    Original Program & SSA Form \\ \hline
	$ \begin{aligned} 
	#1
	\end{aligned}  $
&
	$ \begin{aligned} 
	#2
	\end{aligned}  $
    \\ \hline
    \end{tabular}
\end{center}
}



\begin{document}
\bibliographystyle{gatech-thesis}


%%
\begin{preliminary}



% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.
\end{preliminary}
%%
\chapter{Introduction}


\newcommand{\naive}{na\"ive\xspace}
\newcommand{\Program}{\ensuremath{P}\xspace}
\newcommand{\Forward}{\ensuremath{\Program^{+}}\xspace}
\newcommand{\Inverse}{\ensuremath{\Program^{-}}\xspace}
\newcommand{\Input}{\ensuremath{I}\xspace}
\newcommand{\Output}{\ensuremath{O}\xspace}
\newcommand{\ExtraOuts}{\ensuremath{S}\xspace}
\newcommand{\OutsS}{\ensuremath{\Outs+\ExtraOuts}\xspace}
\newcommand{\Var}{\ensuremath{v}\xspace}
\newcommand{\Vars}{\ensuremath{V}\xspace}
\newcommand{\Exec}[4]{\ensuremath{{#1}\{{#2}={#3}\} \rightarrow \{{#2}={#4}\}}\xspace}


%\newcommand{\vmu}{\ensuremath{v_{in}^I}\xspace}
%\newcommand{\vinit}{\ensuremath{v_{in}}\xspace}
%\newcommand{\veta}{\ensuremath{v_{\eta}}\xspace}
%\newcommand{\vfinal}{\ensuremath{v_{out}}\xspace}
%\newcommand{\vmup}{\ensuremath{v_{\mu}'}\xspace}
%\newcommand{\viter}{\ensuremath{v_{out}^I}\xspace}
%\newcommand{\viterp}{\ensuremath{v_{iter}'}\xspace}
%\newcommand{\mufunc}{\ensuremath{v_{in}^I=\mu(v_{in},v_{out}^I)}\xspace}
%\newcommand{\etafunc}{\ensuremath{v_{out}=\eta(v_{in}^I)}\xspace}
\newcommand{\varmbox}[2]{\ensuremath{{#1}_{\tiny\mbox{#2}}}}
\newcommand{\vmu}{\ensuremath{\varmbox{v}{in}^I}\xspace}
\newcommand{\vinit}{\ensuremath{\varmbox{v}{in}}\xspace}
\newcommand{\veta}{\ensuremath{\varmbox{v}{\eta}}\xspace}
\newcommand{\vfinal}{\ensuremath{\varmbox{v}{out}}\xspace}
\newcommand{\vmup}{\ensuremath{\varmbox{v}{\mu}'}\xspace}
\newcommand{\viter}{\ensuremath{\varmbox{v}{out}^I}\xspace}
\newcommand{\viterp}{\ensuremath{\varmbox{v}{iter}'}\xspace}
\newcommand{\mufunc}{\ensuremath{\varmbox{v}{in}^I=\mu(v_{in},v_{out}^I)}\xspace}
\newcommand{\etafunc}{\ensuremath{\varmbox{v}{out}=\eta(v_{in}^I)}\xspace}
\newcommand{\Loop}{\ensuremath{L}\xspace}



In this thesis, we consider the problem of synthesizing program inverses for imperative languages. 
Specifically, we will build a compiler framework named \emph{Backstroke} \cite{vulov2011backstroke} that can generate a reversible  program and an inverse program for a given input program, by employing several intermediate representations and  program analyses.   

Our primary motivation comes from optimistic parallel discrete event simulation (OPDES). There, a simulator must process events while respecting logical temporal event-ordering constraints; to extract parallelism, an OPDES simulator may speculatively execute events and only rollback execution when event-ordering violations occur \cite{jefferson1985virtual}. In this context, the ability to perform rollback by running time- and space-efficient reverse programs, rather than saving and restoring large amounts of state, can make OPDES more practical. Synthesizing inverses also appears in numerous software engineering contexts, such as debugging, synthesizing “undo” code, or even generating decompressors automatically given only lossless compression code.

This dissertation mainly contains three chapters. In Chapter II, we focus on handling programs with only scalar data and arbitrary control flows. By building a value search graph (VSG) that represents recoverability relationships between variable values, we turn the problem of recovering previous values into a graph search problem. 
The VSG is built based on Static Single Assignment (SSA) \cite{cytron1991efficiently}. 
Forward and reverse programs are generated according to the search results. For any loop that produces an output state given a particular input state, our method can synthesize an inverse loop that reconstructs the input state given the original loop's output state. The synthesis process consists of two major components: (a) building the inverse loop's body, and (b) building the inverse loop's predicate. Our method works for all natural loops, including those that take early exits (e.g., via breaks, gotos, returns). 

In Chapter III we extend our method to handling programs containing arrays. Based on Array SSA \cite{rus2006scalable}, we develop a modified Array SSA from which we could easily build equalities between arrays and array elements. Specifically, to represent the equality between two arrays, we model array subregions explicitly. During the search those subregions will be calculated to guarantee that all array elements will be retrieved. We also develop a demand-driven method to retrieve array elements from a loop, in which each time we only try to retrieve an array element from an iteration if that element has not been modified in previous iterations. To ensure the correctness of each retrieval, the boundary conditions are created and checked at the entry and the exit of the loop. 

In Chapter IV, we introduce several techniques of handling high-level constructs of C++ programs, including virtual functions, copying a C++ object, C++ STL containers, expressions with several side effects, inter-procedural function calls, etc. Since C++ is an object-oriented (OO) language, our discussion in this chapter can also be extended to other OO languages like Java.


\begin{comment}
\section{An introduction example}

We show a simple example here to give an intuition of the program inversion and how our method improves the generated result comparing to other methods. 
Below is a function in C/C++ as a program with the input variables \texttt{a} and \texttt{b}.


\begin{lstlisting}
void foo() {
    if (a == 0)
        a = 1;
    else {
        b = a + 10;
        a = 0;
    }
}
\end{lstlisting}

The inverse of this program should recover the input values of  \texttt{a} and \texttt{b} from their output values of this program.
The simplest inversion program just stores the values of \texttt{a} and \texttt{b}


\end{comment}



\section{Related work}
\label{sec:related-work}
Most of the work on inverting arbitrary (non-injective) imperative programs has focused an incremental approach: the imperative program is essentially executed in reverse, with each modifying operation in the original execution being undone individually.
For example, if statements $s_{1} s_{2} \dots s_{n}$ are executed in the forward directions, the reverse function executes statements $s_{n}^{-1} \dots s_{2}^{-1} s_{1}^{-1}$.
The incremental approach cannot handle unstructured control flows and is difficult to apply in the presence of early returns from functions; the approach presented in this thesis suffers from neither of these shortcomings.  
Furthermore, incremental inversion restores the initial state by restoring every intermediate program state between the final state and the initial state, even though these states are not needed

\paragraph{Syntax-directed approaches} Among the incremental inversion approaches, syntax-directed approaches apply only statement-level analysis. 
If an assignment statement is lossless, its inverse is used: for example, the inverse of an integer increment is an integer decrement.
Otherwise, the variable modified in the assignment has to be saved.
It also provides the ability to record the control flows in the original program, so that in the reverse program the control flows can be reconstructed. An early example of syntax-directed incremental inversion is Brigg's Pascal inverter \cite{Briggs1987}. 
This approach was later extended to C and applied both to optimistic discrete event simulation \cite{Carothers1999} and reversible debugging \cite{Biswas1999}.
Because this approach does not include any program analysis, the  produced result is far from optimized. 
It also has many restrictions.
For example, it cannot handle unstructured programs, loops with early exits, and arrays, among other program constructs.

Consider the specific instance of the Reverse C Compiler (RCC) \cite{Carothers1999}.
In their method, if a state variable is modified by a constructive operation like \texttt{++} or \texttt{+=}, then in the reverse program that state variable can be recovered without state saving. 
For example, if the state variable \texttt{s} is modified by \texttt{s += t}, then it can be recovered by \texttt{s -= t}.
Otherwise, the state saving will be performed to that state variable before it is modified in the forward program. 
The RCC requires that all statements in the original program contain simple expressions, and that the program is structured. 
Because no program analysis is involved, the generated result may be far from optimized.
For example, a state variable may be modified several times in the program, in which case one state saving is enough before its first modification to recover that value. 
But RCC may insert several state saving statements that consume more memory space.
By contrast, in our approach we use program analysis to determine that this state variable is stored only once.
In addition, RCC recovers every variable that is modified in the original program. This approach may lead to restoring many variables not actually related to state variables. By contrast, our method considers just the state variables.
Additionally, we improve on RCC in that  we can handle arbitrary control flows whereas RCC can only handle structured programs. 
This is because RCC relies on proper shapes of the original program to build the reverse program.



\paragraph{Value graph based approach} Akgul and Mooney also previously improved on RCC and related methods. Their incremental inversion algorithm uses def-use analysis to invert some assignment statements that are not lossless \cite{Akgul2004a}; we refer to this approach as regenerative incremental inversion.
In order to reverse a lossy assignment to the variable \texttt{a}, such as \texttt{a = 0}, the regenerative algorithm looks for ways to recompute the previous value of \texttt{a}. 
One technique to obtain the previous value of \texttt{a} is to re-execute its definition. For example, suppose that before \texttt{a} is modified, its previous definition is \texttt{a = b + c}. Then, we may retrieve its old value by redefining it using the same expression: \texttt{a = b + c}. 
Note that in OPDES, the variables we want to retrieve are normally the input of the program that do not  have ``previous'' definitions. 
Therefore, this technique only works on those local variables which are used to retrieved state variables.
Another technique is to examine all the uses of \texttt{a} and see if the value of \texttt{a} can be retrieved from any of its uses. 
For example, if \texttt{a} is used to defined another variable \texttt{b} by \texttt{b = a + c}, then we could retrieve \texttt{a} from \texttt{a = b - c}.
Note that this technique only works for naturally invertible operations like plus and minus.
These two techniques are applied recursively whenever a modifying operation is to be reversed; if they fail to produce a result, the overwritten variable is saved during forward execution.
The specific program analysis builds a graph called modified value graph (MVG),  from which the desired value may be retrieved by performing a particular search on the graph.
In Akgul and Mooney's technique, control flows are handled using the $\phi$-functions produced when computing the program's SSA form \cite{Cytron1991}. However, this approach cannot handle loops.
Our approach is inspired from their method but takes advantage of all the def-use relationships utilized by regenerative inversion, without suffering from the drawbacks of incremental inversion. 
In addition to def-use information, our approach  also derives equality relationships between variables from the outcome of branching statements that test for equality or inequality.
Furthermore, our method can take care of arbitrary control flows, including loops.


A related line of work is inverting programs that are injective, without using any state saving.
Such work tends to focus on inverting functional programs \cite{Abramov2002a,Gluck2005,Kawabe2005}.
Approaches to inverting imperative programs include translation to a logic language \cite{Ross1997}.
By contrast, our method uses no such translation, and is therefore can be applied on imperative languages.

\paragraph{Template based program synthesis framework} The PINS framework for program inversion \cite{Srivastava2011} is a template-based program synthesis framework.
Rather than compiler transformations, as in our approach, PINS uses sketching and synthesis.
In PINS, a programmer defines a template of the inverse with holes that a synthesizer attempts to fill in from a pool of candidates, using the underlying machinery of satisfiability (SMT) solvers.
However, the current incarnation of PINS has weaknesses relative to our work.
First, it is restricted to injective programs.
Since we permit synthesis of a forward program, we can handle both injective and naturally non-injective programs.
Secondly, it is only semi-automated, requiring both programmer annotations and templates.
Our method is fully automated, relative to the limitations on aliasing alluded to previously.
Thirdly, the synthesis time in PINS can be quite long and hard to predict, even for very small programs.
Our method's transformation time is consistent with that of  traditional compilation.
Lastly, PINS does not guarantee a correctly synthesized inverse;
one must apply a verification tool, such as a bounded model checker, to check the synthesized result.
Our method produces correct inverses by construction. Collectively, these advantages make Backstroke more practical for production use than PINS.



\section{Contributions}
\label{sec:contributions}

This thesis has mainly three contributions:

\begin{itemize}
\item In this thesis, we propose a novel automated method to generate forward and reverse programs for a given program. 
Most previous research on program inversion focuses on generating  inverses from injective programs, but we can also handle non-injective programs by generating an injective version of it through storing necessary informations, which expands the scope of program inversion.
By introducing a cost model to our method, we try to minimize the amount of information to be stored in the forward program. 

\item We developed a novel method to handle programs with arrays, with the help of  array subregions and a modified version of Array SSA \cite{rus2006scalable}.
And the method we developed could also be used to handle object accesses by transforming them into array accesses.


\item  Our compiler can make use of human knowledge and generate better results. We feed those programmer-supplied information to our compiler and in practice the generated result are improved. 
We believe that based on this concept we could develop a framework or even a new language for program inversion for better results.
 


\end{itemize}


\section{Limitations}
\label{sec:limitations}

Though Backstroke improves on prior work, it is not without limitations.
Our method cannot handle programs with aliasing, and hence excludes some data structures like linked data structures. 
We will not apply any inter-procedural  analysis in our method. 
For any function calls in the programs, we just assume there is no aliasing in the callee. 
In the last chapter, we will discuss several strategies to handle function calls.
On the specific language level, in the last chapter we will discuss how to handle some C++ constructs, but Backstroke cannot handle  all C++ language features.
Those C++ features we cannot handle are listed below.

\paragraph{Aliasing} Aliasing commonly exists in almost all imperative languages.
In C++, aliases are brought by pointers and references, where specific data may be referenced by more than one pointers or references, and at compile time it is usually very difficult or even impossible to resolve all those aliases.
In addition, the inter-procedural analysis is also heavily depending on aliasing analysis, as procedure calls usually have parameters passed by references.

Our method heavily depends on an intermediate representation called \emph{Static Single Assignment (SSA)} \cite{cytron1991efficiently}, and it is difficult to build it for programs with aliasing. 
Our compiler also lacks of well formed aliasing analysis framework.
Therefore, we will not discuss aliasing in this thesis and will assume there is no aliasing in the programs we handle. 

\paragraph{Subset of C++ language} 

We only handle a subset features of C++ language and those we cannot handle mainly include:

\begin{itemize}
\item Dynamic memory allocation and release.
\item I/O and system calls.
\item Exception handling.
\item Template classes and functions.
\item Function pointers.
\end{itemize}


In Chapter~\ref{chapter:scalar} and \ref{chapter:arrays-loop-free}, we will restrict the target language to a subset of C++ language with following features:

\begin{itemize}

\item Each program is a C++ function with input and output variables which can be recognized by Backstroke. 

\item The type of each variable is either a basic scalar type (e.g. \texttt{int}, \texttt{float}, etc.), or an array type in which each element has a basic scalar type.

\item Each scalar variable can only be modified by assignment operations. An array can only be modified by modifying one of its elements at a time also by assignment operations.

\item All arithmetic operations and logical operations can be performed on variables or expressions. 

\item All control flow statements (except \texttt{throw}) can be used in the programs, including \texttt{if}, \texttt{else}, \texttt{switch}, \texttt{for}, \texttt{while}, \texttt{do while}, \texttt{goto}, \texttt{return}, \texttt{continue}, \texttt{break}.

\end{itemize}

In Chapter~\ref{chapter:cpp}, we will discuss more constructs in C++ and extend our target language to including objects access, function calls, and polymorphism.

\begin{comment}

\section{Preliminaries}

\paragraph{Static Single Assignment Form (SSA)}

SSA is a commonly used intermediate representation in compilers to speed up data flow analysis \cite{Cytron1991}. 
For programs in SSA form, variables are versioned so that each variable with a specific version has exactly one reaching definition. 
Where distinct definitions of a variable merge at confluence points in the control flow graph (CFG), a $\phi$ function is introduced to merge each of the reaching definitions at that point, and this $\phi$ function is also treated as a new definition. SSA has been successfully applied to many compiler analyses. 

\paragraph{SSA Graph}

 An SSA graph \cite{Alpern1988}\cite{Cooper2001}, built based on SSA form, consists of vertices representing operators, function symbols, or $\phi$ functions, and directed edges connecting uses to definitions of values. It shows data dependences between values in a program.

\end{comment}


\chapter{Synthesis for programs with only scalars}
\label{chapter:scalar}

In this chapter we will setup the program we are handling and introduce the framework of our method, which will also be used in the following chapters.
As the first step, we will discuss how to handle programs with only scalars and arbitrary control flows. 
Two important intermediate representations will be introduces: a \emph{value search graph} represents all equality relations in the program, and a \emph{route graph} shows the data dependences in the reverse graph which is built as the search result on a value search graph.
In addition, we will first consider loop-free programs, as its control flow paths are finite and much easier to represent. 
For programs with loops, we will treat each loop body as a subprogram so that we can apply the same approach to handling loop-free programs.
 


%============================================================
\section{Handling loop-free programs}
\label{sec:scalar-loop-free}
\input{scalar-loop-free}

%============================================================
\section{Handling programs with loops}
\label{sec:scalar-loops}
\input{scalar-loops}


%============================================================
%============================================================


\chapter{Synthesis for programs with arrays}
\label{chapter:arrays-loop-free}

In this chapter, we extend the previous method to handling arrays in both loop-free programs and those with loops.
We will still use the same framework. 
That is, we will build a VSG for the program with arrays, then perform a search on it to build a RG.
Then we translate the RG into the source code.
%There are two motivations for us to handle arrays: in OPDES, storing an array is very expensive. We need to improve the performance by using as less state savings as possible on arrays. If we can avoid performing state saving on an array that is modified in a loop and can be retrieved by another loop in the reverse program, the performance will be improved; 
%we also consider to generate the inverse for injective programs, such as lossless compression and encryption,  when state saving is not allowed (because the program is already injective or reversible, it is not necessary to generate a forward program). 
%Arrays are a particularly important case: naively saving the entire array too frequently will be very expensive in space and time. Our method permits saving of just the elements that have changed, and better yet, can often use cheaper computation in place of state saving.
To build the VSG for arrays, we apply a modified Array SSA based on \cite{rus2006scalable}, and define several special VSG nodes for arrays. 
To represent the equality between two arrays, we employ the array subregion as the constraint. 
During the search those subregions will be calculated to guarantee that all array elements will be retrieved. We also develop a demand-driven method to retrieve array elements from a loop. 

%============================================================
\section{Handling loop-free programs with arrays}
\label{sec:array-loop-free}
\input{arrays}

%============================================================
\section{Handling programs with loops and arrays}
\label{sec:array-loops}
\input{array-loops}



\begin{comment}
\chapter{Synthesis for programs with arrays}

Usually data-flow analysis for arrays is more difficult than that for scalars. 
Unlike an scalar, each element in an array does not have a static unique name, but is represented as an array name and an index. 
Two variables as indices of the same array with the same value bring aliases to the indexed element, and this aliasing is difficult to be detected at compile time. 
Fortunately, with the help of the Array SSA \cite{Rus} proposed for programs with arrays, we could build the VSG for such programs and develop a searching rule in order to generate a valid RG. 
The Array SSA tries to match definitions and uses of partial array regions, and we will also employ such array regions in our VSG as another condition of equalities between arrays (the condition we previous used is  control flow path set). 
As a result, we can still reuse the whole framework we previously proposed  to handle programs with arrays.

In this chapter, we first introduce the Array SSA, then describe how we make use of it and add array nodes and edges in the VSG. Then we still divide the programs with arrays into two categories: programs without and with loops. 
We will show how we handle loop-free programs, and propose a method to deal with more challenging programs with loops.
%In this chapter we will use an Array SSA proposed in [] in which concrete array regions are employed.
%Based on this Array SSA, we could build the VSG with special array nodes inside.
%By modifying the searching rules on the VSG, we will see that we could handle programs with arrays using the same framework as before. 


\section{Array SSA}

In the original SSA, when an element of an array is modified, the array itself is assigned with a new version, and the whole array is killed by that definition. 
In Array SSA, the definition of an array element only kills the previous definition of that element represented by a subregion but not the whole array. 
This new SSA form representation can accurately represent the use-def relations between array regions. 

There are three special $\phi$ functions defined for arrays: a $\delta$ function accounts for a partial kill after an element of the array is defined; a $\mu$ function is defined at the beginning of the loop header just as it for scalars; a $\eta$ function defined at the exit of a loop as the output of the loop. 
%a $\gamma$ function defined at the join node in CFG joining definitions of arrays from different control flow paths.

%there is one special $\phi$ function (called a $\delta$ function) added just after the definition, from which a partial kill is represented. For example:


\subsection{$\delta$ function}
 
 When an array element  is modified, except renaming the array's name (assigning it with a new version), a $\delta$ function is defined just after the definition to that element. 
The $\delta$ function summarizes the data-flows of all elements in the array and builds accurate def-use relations with the help of array regions.
 The syntax of a $\delta$ function is shown below:
 $$[a_n, @a_n] = \delta(a_0, [a_1, @a_1^n], [a_2, @a_2^n],  ... , [a_m, @a_m^n] )$$
 $$where \; @a_n=\bigcup_{k=1}^m@a_k^n \; and \; @a_i^n \cap a_j^n = \emptyset, \forall 1\le i,j \le m, i \ne j$$

In the equation above, $@a_k^n$ represents the array region in which the definitions of all elements of $a_k$ are not killed before the definition of $a_n$.
 The code below shows two $\delta$ functions after the definitions of two array elements respectively.
 \begin{flalign*} 
& int \; a_0[N], i_0, j_0; \\
& a_1[i_0] = ...;\\
& [a_2, @a_2] = \delta (a_0, [a_1, @a_1^2]); \\
& a_3[j_0] = ...;\\
& [a_4, @a_4] = \delta (a_0, [a_2, @a_2^4], [a_3, @a_3^4]); 
\end{flalign*} 
where $@a_1^2=@a_2=\{i_0\}, @a_2^4=\{i_0\}-\{j_0\}, @a_3^4=\{j_0\}, @a_4=\{i_0\}\cup\{j_0\}$. Note each array region is represented by a set including symbols (constants or SSA names). We will discuss the representation of array regions later.

In this example, after the definition in the second line, there is a $\delta$ function that defines a new version of the array $a$ as $a_2$. 
%In addition, each argument of the $\delta$ function is a definition of the array with a region. 
The argument $ [a_1, @a_1^2]$ of the $\delta$ function means that all elements of $a_2$ in the region $@a_1^2=\{i_0\}$ are defined by $a_1$. 
The first argument $a_0$ has the definitions of the elements in the region not belonging to other arguments.
 %Each argument except the first one is a tuple $[a_n, @a_n]$ including a definition of the array and the corresponding array region. 
% All elements in this region The $\delta$ function summarizes the reaching definitions of all elements: after one of them is redefined: 

%In this example, after the definition in the second line, there is a $\delta$ function that defines a new version of the array $a$ as $a_2$. In $a_2$, only the value indexed by $i$ equals that in $a_1$, and any other value equals the corresponding one in $a_0$. This means only the $i$th value of $a_0$ is killed by the definition of $a_1$.


\subsection{$\mu$ and $\eta$ functions}

Preiously when we handle loops with scalars, we introduced a $\mu$ function for each variable modified in the loop. 
The arguments in the $\mu$ function come from outside and inside of the loop, and each definition of the $\mu$ function kills all previous definitions. 
In Array SSA,  for each array modified in the loop we also define a $\mu$ function in the loop header receiving definitions from outside and inside of the loop. However, like a $\delta$ function, a $\mu$ function only kills the definition in some regions of the array, and the region is related to the loop index. Let $i$ be the loop index, then all regions in a $\mu$ function are functions of $i$. The syntax of a $\mu$ function is defined by the following equation.
 $$[a_n, @a_n] = \mu(a_0, (i=1,p), [a_1, @a_1^n(i)], [a_2, @a_2^n(i)],  ... , [a_m, @a_m^n(i)] )$$

In this equation the sets $@a_k^n(i)$ are functions of the loop index $i$ and they represent the sets of memory locations defines in some iterations $j<i$ by definition $a_k$ and not killed before reaching the beginning of iteration $i$. 
For any array element defined by $a_k$ in some iteration $j < i$, in order to reach iteration $i$, it must not be killed by other definitions to the same element. 
There are two kinds of definitions that will kill it: definitions ($Kill_s$) that will kill it within the same iteration $j$ and definitions ($Kill_a$) that will kill it at iterations from $j +1$ to $i-1$.
The definition of $@a_k^n(i)$ is shown below.
 $$@a_k^n(i)=\bigcup_{j=1}^{i-1}\left[@a_k(j)-\left(Kill_s(j) \cup \bigcup_{l=j+1}^{i-1}Kill_a(l)\right)\right]$$
 $$ where \; Kill_s = \bigcup_{h=k+1}^m @a_h, \; and \; Kill_a=\bigcup_{h=1}^m@a_h$$

%\section{Arrays in the VSG}

\section{Handling arrays in loop-free programs}

\subsection{Array region representation}

%Unless explicitly indicated, when we retrieve an array, we are retrieving all elements of it. 
Given a C/C++ style array $a[N]$ with length $N$, then all elements can be represented by an interval $[0,N-1]$, or $[0:N-1]$. 
We will use this interval as a set that includes all integers in that interval, and hence an array region becomes an integer set.
%Here $[0,N)$ represents a set containing all integers in this interval. 
%We will use this representation through this report. 
$[0:N-1]$ represents the \emph{universal set} of all regions of the array $a$, denoted by $\mathcal{A}(a)$. 
%Instead of using $@a$, we use $R$ to represent a region in an array. 

Now we define some basic array regions that are commonly used later:
%Among all its subsets, there are some special ones: 

\begin{itemize}
\item The region containing only one integer is represented by $\{i\}$, where $i$ is a constant or an SSA name. 
\item The complementary set $\overline{\{i\}} = \mathcal{A}(a)-\{i\}$ represents all other elements except the $i$th one. 

\item The triplet representation $[a:b:c]$ represents all integers between $a$ and $c$ with stride $b$. If $b=1$, it can be simplified as $[a:c]$. 

\end{itemize}

All set operations can be performed on array regions. Actually, we will rely on those operations to determine if all elements of an array are retrieved during the search on the VSG. 
In order to build a more accurate RG, we need to simplify an expression with set operations.
The process of simplification  depends on the properties of set operations, including identity laws, domination laws, idempotent laws, commutative law, associative laws, distributive laws, etc..
In addition, since an array region set may contain SSA names. 
The knowledge of the relations between them can also help to simplify an set expression.
For example, $\{i\} \cup \{j\} = \{i\} =\{j\}$ if $i=j$, and $\{i\} \cap \{j\} = \emptyset$ if $i\ne j$.
%For example, $\{i\} \cup \overline{\{i\}} = [0,N)=U$. 

%For induction variables in a loop (assume it has depth one), we use two regions for arrays indexed by an induction variable: one for the local iteration, and one for a summary for the scope outside of the loop. 

\subsection{Arrays in the VSG}

For each definition of an array we add a special node in the VSG as its representation and we call it an \emph{array node}. 
Any element of an array is a scalar and is still denoted by a normal value node. 
%Remember that every edge in a VSG is attached with the path information.
For any edge between two array nodes $a_x$ and $a_y$, an array region $R$ is attached to it, showing that $a_x$ and $a_y$ have the same values for all elements in the region $R$.
For each access of an array, e.g. $a_x[i]$, we also add a region $\{i\}$ on the edge between VSG nodes representing $a_x$ and $a_x[i]$.

We will build the VSG for arrays based on the Array SSA with some modifications. 
%Let's see first how we build arrays nodes for a $\delta$ function. 
In Array SSA, a $\delta$ function is always defined under a definition to an element of it, and those two definitions create two names of the same array. 
To reduce the number of array nodes in VSG, we combine those two names into one.
%, and connect this array node to other array nodes and the definition of its element. 
For example, for the following code:
\begin{flalign*} 
& int \; a_0[N]; \\
& a_1[i] = ...;\\
& [a_2, @a_2] = \delta (a_0, [a_1, @a_1^2]); \\
& a_3[j] = ...;\\
& [a_4, @a_4] = \delta (a_0, [a_2, @a_2^4], [a_3, @a_3^4]); 
\end{flalign*} 
We will transform it into the code below:
\begin{flalign*} 
& int \; a_0[N]; \\
& a_1[i] = ...;\\
& a_1 = \delta ([a_0, \overline{\{i\}}], [a_1, \{i\}]);  \\
& a_2[j] = ...;\\
& a_2 = \delta ([a_1, \overline{\{j\}}], [a_2, \{j\}]); 
\end{flalign*} 

After the definition of $a_1$, instead of creating a new name $a_2$, we maintain a correct equality relations between $a_1$ and $a_0$, and also $a_1$ and $a_1[i]$. 
In the VSG, we  add an edge with region $\overline{\{i\}}$ between nodes of $a_0$ and $a_1$, and  add another edge with region $\{i\}$ between node of $a_1$ and $a_1[i]$. 
With the help of this region information, we can make a full kill to $a_0$ when $a_1$ is defined (that is, $a_0$ will never be used after the definition of $a_1$). 
Similarly, when $a_2[j]$ is defined, we create another $\delta$ function that shows the equality between $a_1$ and $a_2$ in the region $\overline{\{j\}}$, and $a_2$ and $a_2[j]$ in the region $\{j\}$.
In this way, a $\delta$ function always has two arguments: one is the newly defined array with the region $\{i\}$ where $i$ is the index of the defined element; one is the reaching definition of the array before defining the element, with the region $\overline{\{i\}}$.
%As a consequence, all later use of $a_2$ will be replaced by $a_1$.
The created nodes and edges in the VSG are shown below:


\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=above:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \path
    (a1) edge node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge node  [lbl, swap] {$\overline{\{j\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    ;
}{The VSG edges with array regions between array nodes.}{fig:array-vsg}

In the VSG, each array node is represented by a square node, being differentiated from circle nodes for scalars. 
For each element access of an array $a_x[i]$, the corresponding value node will be connected to the array node for $a_x$ with region $\{i\}$.
%The region informations are attached to edges incident to array nodes.

In Array SSA,  $\phi$ functions\footnote{In the original paper of Array SSA, it is called a $\gamma$ function.} defined for arrays at a join node in the CFG has the same meaning as those for scalars.
%there is also a for an array if it has different definitions from incoming edges. In this case, we take it in the same way as scalars. 
Therefore, in the VSG we build a $\phi$ array node representing the array defined by the $\phi$ function and connect it to all of its arguments with correct control flow path set and  the full array region. 






%Note that an array can only be modified through each of its elements.

\subsection{Search the VSG to retrieve an array}

The searching rules for arrays on the VSG is similar to that for scalars, except we have to take the array region into account.
In the VSG, every edge incident to an array node has both control path information and array region information. 
During the search, once such an edge is selected, the search begins to have an addition goal to retrieve all elements of the corresponding array in the region on that edge.
Only when the search reaches available array nodes or scalar value nodes can the region goal added to the search be removed.
At the beginning of the search, if an array node is the target node, then the search needs to retrieve all elements of the array.
%The special searching rules for arrays:

During the search on the VSG, it is possible that an edge is selected several times in searching for more than one values with different control flow paths and array regions. 
Therefore, in the RG, the control flow path and array region sets are paired as $[P,R]$, where $P$ and $R$ stand for the path set and array region set respectively.
 And it is possible that an edge $e$ has several such pairs $[P_0,R_0],[P_1,R_1],...,[P_n,R_n]$.
We require that $P_i \cap P_j = \emptyset, i\ne j$.
Then for the edge $e$ we can get those sets through $P_i(e)$ or $R_i(e), i = 0,...,n$.

As previously, the searching rule should guarantee the search result RG to meet several properties. 
For each edge $e$ in the RG, let $P(e)$ denote the control flow path set on it, and let $R(e)$ denote the array region set on it.
Those properties include:

\begin{enumerate}[I)]

\item For each target array node $n$ of the array $a$, and for each control flow path $p$,
	$$\bigcup_{\substack{\mathit{out} \in \text{OutEdges}(n)  \\  p \in P_i(out)}}R_i(\mathit{out}) \quad = \quad \mathcal{A}(a) $$ 
	\label{rg-property-1}

\item For each array node $n$ that is neither a target node nor an available node, and for each control flow path $p$,
	$$\bigcup_{\substack{\mathit{out} \in \text{OutEdges} (n) \\ p \in P_i(out)}} R_i(out) = \bigcup_{\substack{\mathit{in} \in \text{InEdges} (n)  \\  p \in P_j(in) }}R_j( \mathit{in} )$$ 

\item For each array node $n$, given any two outgoing edges $n\to s$ and $n\to t$, for each control flow path $p$, and $p \in P_i(n \to s)$, and $p \in P_j(n \to t)$, then $R_i(n\to s) \cap R_j(n\to t) = \emptyset$.

\item If $e$ is a route graph edge and its corresponding edge in the VSG is $e'$, then for each $R_i(e)$,
$R_i(e) \subseteq R(e')$.
	\label{rg-property-4}

%\item For each directed cycle with edges $e_1 \dots e_n$,  $\quad \bigcap_{i=1}^n{P(e_{i})} = \emptyset$
	%\label{rg-property-5}

\end{enumerate}




%1. For each edge incident to an array node, except the path information, there is also region information. 

%2.  If the search starts from an array node, it tried to retrieve all elements of that array. Then the region $[0,N)$ is attached to this search. This is very similar to the path information. 

%3. When the search reaches an element node from an array node, the region of the search can be removed. The search from node a to node b has the region information iff a or b is an array node.

%4. In the RG, for each array node, R(in)=R(out).


\subsection{State saving on an array and its elements}

In the VSG, to enable the capability of generating state saving statements in forward program, there is still a state saving node, which is connected to all scalar value nodes as well as array nodes. 
Each state saving edge connected to an array nodes of the array $a$ has the region   $\mathcal{A}(a)$.
When the search reaches a state saving edge incident to an array node, the array region on it will be calculated and updated according to the searching rule. 
As a result, it is possible that the final region on this state saving edge is unknown at compile time or is represented by an expression with several set operations.
To facilitate the code generation, we require that the region on a state saving edge must fall into two categories: a single element, or the full region. 
In the first case, we only need to store a single element of the array; in the second case, we will store the whole array by building a loop, and the size of the loop needs to be retrieved before storing the whole array. 


\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=below:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \node [scalar] (ss) at (9,0) {SS};
    \path
    (a1) edge node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge node  [lbl, swap] {$\overline{\{j\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    (a0) edge [bend left] (ss)
    (a1) edge [bend left] (ss)
    (a1i) edge (ss)
    (a2j) edge (ss)
    ;
}{A state saving node connecting all value nodes.}{fig:array-ss-vsg}

\Drawgraph{
    \node [array, label=above:$a_0$] (a0) at (0,0) {};
    \node [array, label=above:$a_1$] (a1) at (3,0) {};
    \node [scalar, label=below:${a_1[i]}$] (a1i) at (6,0) {};
    \node [array, label=below:$a_2$] (a2) at (3,-3) {};
    \node [scalar, label=below:${a_2[j]}$] (a2j) at (6,-3) {};
    \node [scalar] (ss) at (9,0) {SS};
    \path
    (a1) edge [pre, red,ultra thick] node  [lbl, swap] {$\overline{\{i\}}$} (a0)
    (a1) edge node  [lbl] {${\{i\}}$} (a1i)
    (a2) edge [pre, red,ultra thick] node  [lbl, swap] {$\overline{\{j\}} \cap \overline{\{i\}}$} (a1)
    (a2) edge node  [lbl] {${\{j\}}$} (a2j)
    (a0) edge [post, red,ultra thick,bend left] node  [lbl] {$\{i\}$} (ss)
    (a1) edge [post, red,ultra thick,bend left] node  [lbl,swap] {$\{j\}\cap \overline{\{i\}}$} (ss)
    (a1i) edge (ss)
    (a2j) edge (ss)
    ;
}{The search result is shown in bold red edges.}{fig:array-ss-rg}


For example, Figure~\ref{fig:array-ss-vsg} shows the VSG for the code we used before, and Figure~\ref{fig:array-ss-rg} is the search result RG shown as bold edges.
The array region on the edge from $a_1$ to $SS$ node is $\{j\}\cap \overline{\{i\}}$.
And $\{j\}\cap \overline{\{i\}} = \left\{ \begin{array}{rcl}  \{j\} & \mbox{if} & i \ne j \\ \emptyset & \mbox{else} \end{array}\right.$. 
If we cannot resolve the result of it at compile-time (that it, we don't know if $i$ and $j$ have the same value), we will use $\{j\}$ instead of $\{j\}\cap \overline{\{i\}} $ in the RG.
In the generated code, we always perform a state saving on $a_1[j]$.

\end{comment}

%\section{Handling arrays with loops}

%\section{Handling object access}

%\section{Handling linked data structure}

%This is what to be proposed in this proposal.


\section{Handling linked data structures}
\label{linked-data-structure}

A linked data structure contains elements which have one or more ``links'' to other elements such that each element could reside in separate memory place from others, comparing to arrays whose elements are always stored together.
Typical linked data structures include linked list, tree, graph, etc..
Since an element can be linked by several links from other elements, aliasing prevalently exists among linked data structures.
But this is similar to the aliasing brought by indices in arrays.

If we treat the address (normally saved in links) of elements in linked data structure as indices in arrays, it is possible to handle those linked data structures using the method we developed for arrays.
Let's take the linked list (we will call it as list briefly) as an example.
Let $Node$ define a node in the list and its definition is shown below:


\begin{lstlisting}
struct Node {
    int val;
    Node* next;
};
\end{lstlisting}

Now suppose we have a list and would like to increment the value in each node by one. 
The code should look like this:

\begin{lstlisting}
void increment(Node* n) {
    while (n != NULL) {
        n->val++;
        n = n->next;
    }
}
\end{lstlisting}

Then we perform a transformation on this code by taking each field of $Node$ as an array and the address of the node as the index.
Here we create two new arrays: $val$ and $next$.
Whenever we access the field with the same name by an instance of $Node$, we transform it into an access to an array.
That is, for $n\to val$ we get $val[n]$.
The transformed code is shown below:

\begin{lstlisting}
void increment(Node* n) {
    while (n != NULL) {
        val[n]++;
        n = next[n];
    }
}
\end{lstlisting}
 
However, unlike the index of an arrays that may have regular access patterns, we usually don't have enough information of the pointers pointing to linked data structures. 
We may use some high-level semantics like during the traversal of a list, each element only appear at most once (otherwise a cycle will exist in the list which breaks the definition of it). 

\chapter{Synthesis for C++ programs}
\label{chapter:cpp}

In prior chapters, we target programs in no specific languages, as our method is general and can be applied on any imperative language.
In this chapter, we introduce some techniques of handling several high-level constructs of C++ programs. 
We choose C++ for three reasons:
first, Backstroke is based on ROSE compiler \cite{quinlan2000rose}, which is a C++ source-to-source compiler; 
second, C++ is an object-oriented (OO) language, and our discussion in this chapter can also be extended to other OO languages like Java;
third, initially Backstroke is used to generate reverse functions for OPDES events, and our  simulation engines (GTNets \cite{riley2003simulation} and ROSS \cite{carothers2002ross}) are writing in C/C++.

\section{Normalizing C++ programs}

Backstroke is a source-to-source translator.
It is based on ROSE,  which is a C++ source-to-source compiler. 
The ROSE compiler parses the source code and transform it into an abstract syntax tree (AST) as the intermediate representation (IR).
The AST preserves almost all syntax informations on source level of the source code, such that it can reproduce the similar code as the input together with the transformations as desired.
But it is not sufficient for some low level analysis, where other IRs like three address code, in which each statement only has one operation with at most three operands, is easier to analyze.
As seen in the prior two chapters, our method heavily depends on SSA form.
Building the SSA form on source code is challenging, as the source may contain complex statements or expressions.

To amortize those problems, given an input program, we will firstly normalize it into another form without changing the behavior of the original program.
We will break some complex expressions or statements into simpler ones, and try to let each statement only have one side effect.
Then the data flow analysis including SSA form can be easily generated.
In addition, because the forward program is generated by instrumenting state saving and path recording statements in the original program, normalizing the program can make it easier to find those instrumentation locations in the program. 
%Therefore, before we generate the SSA form, we first normalize the AST into a regular one.

\subsection{Forcing a specific execution order of several expressions}
%1. If the execution order of two expressions are not defined, we force a specific order.

In C++, there is a concept called \emph{sequence point} that is used to indicate the execution order of several expressions. 
A sequence point defines any point in a computer program's execution at which it is guaranteed that all side effects of previous evaluations will have been performed, and no side effects from subsequent evaluations have yet been performed.
Normally, all expressions in a statement are executed before the next statement. 
But in one statement, several expressions with side effects may exist. 
In this case, the C++ standard  only guarentees the execution order for the following expressions:

\begin{itemize}

\item Comma expression: $expr1, expr2$. 
In this expression, $expr1$ must be executed before $expr2$, and the returned value of $expr2$ is used as the returned value of this comma expression.

\item Logical and/or operator: $expr1 \;\&\&\; expr2$ / $expr1\; ||\; expr2$. 
In this expression, $expr1$ must be executed first, and $expr2$ is  executed only if the returned value of $expr1$ is $true$/$false$ for logical and/or operations.
This is called \emph{short-circuit evaluation}.

\item Conditional operator: $expr1 \;?\; expr2 : expr3$. In this expression, $expr1$ will be executed first, and if the returned value is $true$, then $expr2$ is  executed; otherwise, $expr3$ is executed.

\end{itemize}

For other expressions with several side effects, their order is undefined. 
For example, if we have $++a = ++b$, the resulted value of $a$ can be either $b$ or $b+1$.
Although there is no syntax error in this expression, it may bring some problems for our data flow analysis.
To remove this ambiguity, we transform this kind of expression into several ones and force an execution order.
The new expression will be $++a, ++b,  a = b$, in which the comma expressions form a specific execution order of all three side effects.




\subsection{Facilitating instrumentations}

A forward program is always generated by instrumenting the original program with state saving and path recording statements.
We have to find the proper place to make the instrumentations.
However, some C++ constructs make it a little difficult to find such a place.

Take the logical and operator as an example.
The evaluation of a logical and operation follows the short-circuit rule: if the operand on the left hand side is evaluated as $false$, then the operand on the right hand side will not be evaluated.
Actually, such an operator implicitly creates additional control flow paths to the program, and during path recording we also have to count those paths, especially if there exists side effects in either operand. 

Below is an example showing this situation.

\begin{lstlisting}
if (a > 0 && b++ > c)
    /* true body */
else
    /* false body */
\end{lstlisting}

The CFG of it is shown in Figure~\ref{Fig:ShortCircuitCFG}.
During the path recording, we may have to insert a statement on the dashed edge in this CFG, which is a critical edge (an edge which is neither the only edge leaving its source block, nor the only edge entering its destination block).
However, at the source level it is very difficult to insert a statement on this edge.


\Drawgraph{
    \node [array,font=\small] (2) at (0,-2) {\texttt{if (a > 0)}};
    \node [array,font=\small] (3) at (0,-4) {\texttt{if (b++ > c)}};
    \node [array,font=\small] (4) at (-2,-6) {\texttt{/* true body */}};
    \node [array,font=\small] (5) at (2,-6) {\texttt{/* false body */}};
    \path
    (2) edge [post] node  [lbl,swap,yshift=-3] {T}(3)
    (2) edge [post, bend left] node  [lbl,yshift=-3] {F}(5)
    (3) edge [post, dashed] node  [lbl,yshift=-3] {F}(5)
    (3) edge [post]  node  [lbl,swap,yshift=-3] {T}(4)
    ;
}{The CFG for a logical and operation \texttt{a > 0 \&\& b++ > c} as a predicate.}{Fig:ShortCircuitCFG}

Our solution is first converting such an operator into a conditional operator, which will be converted into conditional statements later.
We convert logical and \& or operations into conditional operation according to the rules as below:
$$expr1 \;\&\&\; expr2  \;\to\;  expr1 \;?\; expr2 : false$$
$$expr1 \;||\; expr2  \;\to\;  expr1 \;?\; true : expr2$$

For a conditional operator as a predicate, we will declare a new boolean variable, then assign the result of the conditional operation to it.
Consequently, the code above becomes:

\begin{lstlisting}
bool f = a > 0 ? b++ > c : false;
if (f)
    /* true body */
else
    /* false body */
\end{lstlisting}

We then go ahead convert the conditional operator into a conditional if statement, and separate the statement $f = b++>c$ into two statements such that each statement only contains one side effect:

\begin{lstlisting}
bool f;
if (a > 0) {
    f = b > c;
    b++;
}
else
    f = false;
if (f)
    /* true body */
else
    /* false body */
\end{lstlisting}

Now the code is much cleaner and instrumentations are easier to made to it.
Note that after this conversion, we get one more control flow path which is a infeasible path (the path passing through the false body of the first branch and the true body of the second branch).
This is fine as the behavior of the program is not changed.


\section{State savings on C++ variables}

In the prior chapters, when we need a state saving, we normally call two functions: $store()$ and $restore()$.
But what is behind those two function calls?
In this section, we will introduce how and where to store variables in Backstroke.
Besides variables of basic types, we will also discuss how to correctly store a C++ object with class type, including the case where the object is referenced by a pointer or reference of its superclass type.

\subsection{The storage stack}

To perform state savings, we need a storage container that provides interfaces to perform the storing and restoring operations. 
Since the state saving made by Backstroke is incremental, the names and number of variables to be stored and the size of space needed are unknown at compile time.
Therefore, in Backstroke we use a stack that stores all variables.
As stack is a LIFO (last-in-first-out) data structure, a variable that is firstly saved and pushed will be popped lastly. 
And during the OPDES, reverse events are also implemented in a LIFO order (the first reverse event is used to remove the side effects produced by the last forward event in the sequence). 
Hence stack is the right container to store variables and we only need one stack for all event handlers.

To let our stack hold variables of arbitrary types, one way in C++ is that for each variable we store a pointer of \texttt{void *} type pointing to the copy of the variable, and during the restoration, the referenced variable should be casted to the proper type.
However, when a great number of variables are stored, the memories possessing their copies may be scattered, and for each variable an additional size information is needed in the heap memory and is usually attached to the memory containing the variable, which can bring significant time and space overhead.
To amortize this overhead, we could use a memory pool that allocates memories to store variables. In addition, we also observed that most stored variables are of basic types like integers and floating points.
To decrease the memory used to store those variables, we create several special stacks which are used to store variables of basic types. 

The storage stack should also provide an interface to clear the items inside.
We can do this by popping all items and release the memory for each item.
However, in some scenarios, instead of clear all items, we may only want to remove some old items but keep new ones, when it is more convenient if we can access the stack from both ends.
In Backstroke, our storage stack has the type \texttt{std::deque<void*>}, which is a double-ended queue that satisfies our needs.
The function $store()$ and $restore()$ are defined as below:

\begin{lstlisting}
std::deque<void*> storageStack;

template<class T>
void store(const T& obj) {
    storageStack.push_back(new T(obj));
}

template<class T>
void restore(T& obj) {
    T* t = static_cast<T*>(storageStack.back());
    storageStack.pop_back();
    obj = *t;
    delete t;
}
\end{lstlisting}

\subsection{Storing C++ objects}

For a variable with basic types or POD (plain old data) types, there are no side effects when copying it during state savings.
However, for a C++ object of class type, its copy constructor, assignment operator, and destructor (we will call them \emph{big three functions} below) are triggered when storing and restoring it.
Backstroke requires that those big three functions should be defined ``correctly''.
Here we discuss the correctness of them.

Given an object $obj$ of type $class \;S$ in the original program, if it is stored and restored in the forward and reverse programs, we have to make sure the state is recovered  after running the reverse program comparing to that  before running the  forward program.
We store an object by calling its copy constructor to create a copy of it which is stored in our storage stack. 
We don't use assignment operator here because the class of the object may not have a default constructor, in which case creating an object is not that straight forward.
When we restore this object, normally the object is already there, and we can just assign the copy to it by calling the assignment operator. 
After the restoration, we delete the stored object that calls its destructor.
We should guarantee that after storing and restoring an object the simulation state is correctly rolled back.
Below are the $store()$ and $restore()$ implementations as shown before with comments showing when those three functions are called.


\begin{lstlisting}
template<class T>
void store(const T& obj) {
    storageStack.push_back(new T(obj));  // The copy constructor is called.
}

template<class T>
void restore(T& obj) {
    T* t = static_cast<T*>(storageStack.back());
    storageStack.pop_back();
    obj = *t;  // The assignment operator is called.
    delete t;  // The destructor is called.
}
\end{lstlisting}


To guarantee the correctness, we require that there is no side effect in the big three functions. 
For instance, each of them should not modify a global or static shared variable that belongs to the simulation state and could affect the simulation result. 
In addition, the copy constructor and assignment operator usually should make a deep copy of the object.
This means if this object has a pointer pointing to another chunk of memory that is managed by this object, this piece of memory should also be copied in those two functions, and needs to be released in the destructor. 
An example is \texttt{std::vector}, whose copy constructor and assignment operator both make a deep copy of the array inside.
However, this requirement may be too strong as sometimes it could conflict with the  behavior of the original program. 
An apparent situation is that the copy constructor does not alway make a deep copy of itself when several instances of this class have links to the same data and any one can update it. 
If the copy constructor makes a deep copy, the behavior of the program would be changed.
In the next part we will propose a solution to handle this issue.



\subsection{Storing an object referenced by a pointer/reference with its superclass type}
%Polymorphism is possibly the most important Object-Oriented (OO) feature in C++ and other OO languages. 
%\TODO{need a definition of polymorphism}
In C++, an object with a pointer or reference type $Base$ may have a concrete type $Sub$ which is a subclass of $Base$.
However, at compile time, it is impossible to identify the real type of such an object.
As a result, calling the copy constructor of the $Base$ class will not do the whole copy of that object, but only the part  that belongs to the type $Base$ only.
This will lead to the error in state savings as shown below:


\begin{lstlisting}
class Base {
    int baseVal;
};

class Sub : public Base {
    int subVal;
};

Base *p = new Sub;
store(*p);  // Error! Only p->baseVal is stored.
\end{lstlisting}


Polymorphism is the solution to this problem.
What we need is the virtual versions of the big three functions.
However, in C++ only the destructor can be declared as a virtual function.

To get rid of this restriction, we can define our own virtual versions of the three big functions. 
Specifically, we require that the class $Base$ should provide two special interfaces as virtual functions: a $clone$ function that returns a copy of the object; an $assign$ function that assigns itself to another object.


\begin{lstlisting}
class T {
public:
    virtual T* clone();
    virtual void assign(T* obj);
};

class S : public T {
public:
    virtual T* clone()
    { return new S(*this); }
    virtual void assign(T* obj)
    {  *this = *(dynamic_cast<S*>(obj)); }
};
\end{lstlisting}

There are two  advantages of those two new interfaces:
first, they are provided as the complementary interfaces to the classes in the original program and hence will not change its behavior, and when the original big three functions cannot fulfill our requirement, we can always defines those two virtual   functions;
second, in this way we could  correctly store an object referenced by a pointer or reference of its superclass.

The code below shows the new $store$ and $restore$ functions using the new interfaces.

\begin{lstlisting}
template<class T>
void store(const T& obj) {
    storageStack.push_back(obj.clone());  // The virtual copy constructor is called.
}

template<class T>
void restore(T& obj) {
    T* t = static_cast<T*>(storageStack.back());
    storageStack.pop_back();
    obj.assign(*t);  // The virtual assignment operator is called.
    delete t;  // The destructor is called.
}
\end{lstlisting}

Note that it is possible that the original destructor could not release all resources generated by the $clone()$ function. 
In this case, the $assign()$ function should release those resources so that after running $assign()$ and destructor, all resources generated by $clone()$  will be safely released.


\section{Handling function calls}

In this section, we describe various techniques for extending our program inversion
framework to handle programs with function calls. 
In addition, we discuss the pros and cons of these techniques.

\subsection{Inlining}

A nave technique of handling multiple function programs is to inline all the functions in the program such that program is effectively converted to a single function.
The existing technique can then be applied to such a program to generate the corresponding forward program $P^+$ and inverse program $P^-$. 
However, there are two major drawbacks of this approach: 1) In programs with recursive calls, inlining can lead to a infinitely large program and thus, recursion needs to be handled in a special manner, 2) Even without recursion, inlining causes a large increase in the program size. 
Consequently, the corresponding VSG generated for this program is a very
large graph. 
Performing the lowest-cost search on this graph for generating the RG then becomes practically infeasible. 
However, inlining does ensure that the most optimal solution compared to the other techniques described later.

\subsection{State Saving}

Another naive but practical approach is to save the incoming state before any function call. 
As a result, when generating the program inverse, it is possible to ignore the function call completely and restore the state saved before the call to the function. Although this method ensures correct programs and allows the application of
the existing techniques, it negates the very purpose of generating inverse programs.
By forcing a state save before every function call, this approach incurs a
large cost.

\subsection{Unique VSG and unique RG for each method}

In this approach, we generate a unique VSG for each non-virtual method. 
Further, considering the input parameters as the input state $I$ and the return value of the method as the output state $O$, we generate a RG for this method. 
Consider a method $foo()$. 
In the VSG of the callee of $foo()$, there exists a node which represents the
call to $foo()$. 
This node is connected to the return value via an incoming edge and has outgoing edges to the parameters of the called method. 
Using this approach, we add a new node in the VSG of the callee representing a call to $foo()^-$. 
The parameters of $foo()$ are connected to $foo()^-$ via edges incoming into $foo()^-$ while $foo()^-$ is connected via an outgoing edge to the return value of $foo()$. 
Further, the cost associated with these edges is as determined by the RG generated for $foo()$. 
This method allows a scalable approach to generating forward and reverse programs. 
Although the final program generated by inlining carries a lower cost
than by this approach, it is possible to use this approach in a practical setting.
However, this approach suffers from one major drawback. 
Suppose that a parameter of $foo()$ is overwritten just before the call to $foo()$.
The value of the parameter needs to be stored in the forward program $P^+$ since it is can not be recovered by computational methods. 
However, if the reverse program use $foo()^-$, it redundantly recovers the parameter value at the call to $foo()$ only for the recovered parameter value to be replaced by the stored value. 
This lack of flexibility in choosing the recovery mechanism for each parameter individually and being forced to use the same mechanism for all parameters leads to a sub-optimal solution.

\subsection{Unique VSG and multiple RG for each method}

This approach resolves the drawback in the previous approach. 
We still generate a unique VSG for each method. 
However, a separate RG is generated for each parameter with the parameter as the input state I and the return value of the method as the output state $O$. 
The separate RGs are then used to generate multiple inverse versions of the original method, with each inverse version generating a single parameter as the return value. 
Consider a method $foo()$ with parameters $a$ and $b$ and return variable $c$. Using this approach, we generate the inverse versions $foo()A^-$ and $foo()B^-$. In the VSG of the callee to $foo()$, there are edges from $c$ to $foo()$,
from $foo()$ to $a$ and from $foo()$ to $b$. 
We additionally add edges from $a$ to $foo()A^-$, $b$ to $foo()B^-$, $foo()A^-$ to $c$ and $foo()B^-$ to $c$. 
This allows us to make independent decisions when generating the reverse code for each parameter of $foo()$.


%\section{Handling virtual functions}


\section{Handling STL containers}

\newcommand{\stlvector}{\texttt{std::vector}\xspace}
\newcommand{\stldeque}{\texttt{std::deque}\xspace}
\newcommand{\stlqueue}{\texttt{std::queue}\xspace}
\newcommand{\pushback}{\texttt{push\_back()}\xspace}
\newcommand{\popback}{\texttt{pop\_back()}\xspace}
\newcommand{\push}{\texttt{push()}\xspace}
\newcommand{\pop}{\texttt{pop()}\xspace}

C++ STL containers are commonly used in the real code, including the simulation programs.
From high level, if we take an STL container instance as a state variable, and if it is modified in the event function, we have to use state saving technique that stores and restores this container.
From the respect of safety, if the type of the container elements has a proper copy constructor and a copy assignment operator,  the copy constructor and copy assignment operator of STL container will call it repeatedly on each element to store and restore the whole container;
from the respect of performance, this method suffers from bad efficiency: even if only one element in the container is modified, we have to copy the whole container during state saving.
Due to the restrictions of our method, we cannot handle the C++ STL container from low level (mass aliasing exists in STL container). 
In this section, we propose a method to ``undo'' some modifications on STL containers better than state saving, with the help of high level information. 


As a basic example, let take a look at the most commonly used STL container: \stlvector. 
A \stlvector behaves like an array, except its size can by adjusted dynamically at runtime.
It has an interface named \pushback which adds an element to the end of the vector, and hence the size of it is incremented by one. 
If a vector as a state variable is changed in this way, it is apparent that we can call its another interface \popback to undo this modification -  a much more efficient way than state saving. 
Now let's consider how to undo a \popback. 
Note that since the last element of the vector is popped and destroyed, we cannot call 
\pushback to undo the pop unless we have a copy of the popped element.
Here we come back to our forward-then-reverse function call approach.
In the forward function of \popback, we make a state saving on the being popped element before popping it, and in the reverse function of \popback, we can just restore the saved element and then call \pushback to add it back to the vector.
The forward and reverse functions of \pushback and \popback are generated by hand but can be recognized by Backstroke. They are shown below:

\begin{lstlisting}
template <class T>
inline void bs_vector_push_back_forward(std::vector<T>& v, const T& t) {
    v.push_back(t);
}
template <class T>
inline void bs_vector_push_back_reverse(std::vector<T>& v) {
    v.pop_back();
}
template <class T>
inline void bs_vector_pop_back_forward(std::vector<T>& v) {
    store(v.back());
    v.pop_back();
}
template <class T>
inline void bs_vector_pop_back_reverse(std::vector<T>& v) {
    T t;
    restore(t);
    v.push_back(t);
}
\end{lstlisting}

Now let's consider \stlqueue, which is  a container adapter that gives the programmer the functionality of a queue - specifically, a FIFO (first-in, first-out) data structure.
It has two interfaces that modify the internal data: \push and \pop.
\push pushes an object to the end of the queue and \pop pops the object at the front of the queue.
However, there is neither an interface that could push an object to the front, nor an interface that could pop an object from the end.
Hence it is impossible to write ``cheap''  reverse functions for \push and \pop.
But an \stlqueue actually contains a real container inside, and the default one is a \stldeque, which has more interfaces that we need.
Therefore, we can simply change the \stlqueue into a \stldeque then use the forward/reverse functions of it. 

The Table~\ref{table:stl} shows how we generate the forward and reverse code for common STL containers and their interfaces.

\begin{table}
\begin{center}
    \begin{tabular}{ | p {4cm} | p {5cm} | p {5cm} |}
    \hline
    \textbf{STL container \& interface} & \textbf{Forward Code} & \textbf{Reverse Code}
    \\ \hline
	vector::push\_back() deque::push\_back() list::push\_back(). & Call push\_back(). & Call pop\_back().
    \\ \hline
    vector::pop\_back() deque::pop\_back() list::pop\_back(). & Store the popped element, and then call pop\_back() & Restore the saved element and call push\_back() to push it to the container.
    \\ \hline
    vector::insert() deque::insert() list::insert()& Store the index/iterator of the element to be inserted and call insert(). & Remove the element with the saved index/iterator by calling erase().
    \\ \hline
    vector::erase() deque::erase() list::erase & Store the element to be erased and its position, then call erase(). & Restored the erased element and insert it to the saved position.
    \\ \hline
    set::insert() map::insert() & Call insert() and store the iterator of the inserted element. & Remove the stored element by calling erase().
    \\ \hline
    set::erase() map::erase() & Store the element to be erased can call erase(). & Insert the stored element by calling insert().
    
    \\ \hline
    stack::push() & Call push(). & Call pop().
    
    \\ \hline
    stack::pop() & Store the popped element, can then call pop().  & Restore the saved element and call push() to push it to the stack.
    
    \\ \hline
    queue::push() & Use a deque instead of a queue, and call deque::push\_front(). & Call deque::pop\_front() to pop the element at the end of the queue.
    
    \\ \hline
    queue::pop() & Use a deque instead of a queue. Save the popped element, and then call deque::pop\_back(). & Call deque::push\_back() to push the saved element to the front of the queue.
    
    \\ \hline

    \end{tabular}
\end{center}
\caption{The forward and reverse methods for some interfaces of STL containers}
\label{table:stl}
\end{table}


\chapter{Conclusion and future work}

In this thesis we presented a compiler framework Backstroke that generates forward and reverse programs for a given program automatically. 
Two novel intermediate representations are critical in Backstroke: a VSG shows equalities between values in the program, and each equality is constrained by a set of control flow paths; a RG is the search result of the VSG which shows all data dependences in the reverse program and the forward and reverse programs are generated from the RG.

After the VSG is built and all target and available nodes are located, we start a search from target nodes until available nodes are reached. A special state saving node is added to the VSG such that any value could be retrieved at least using the state saving method, though normally it is more expensive than other methods. 
The search algorithm tries to get the search result with the least cost - that is, minimize the usage of state savings.
It also guarantees each target value is retrieved on all control flow paths in the program. 
To deal with arrays, we introduce special array nodes into the VSG and show equalities between subregions of arrays. 
An array subregion is a subset of array elements, and during the search for a specific subregion, we perform set operations on subregions to make sure the target subregion of the array is retrieved. 
To retrieve an array subregion from a loop, we developed a demand-driven algorithm that retrieves one array element from each iteration of the loop.

To handle loops, we first transform the loop into a single-entry single-exit loop like a while loop or do-while loop.
Given a while loop, the key idea behind retrieving an input value of the loop from its outputs is to retrieve the corresponding input value in each iteration. 
We need to augment the VSG with additional edges that connect the loop's input and output values to the input and output values of the body.
We may then run the search procedure, which yields an RG with cycles.
The presence of cycles indicates that we need to build loops in the reverse program.
To construct such a loop in the reverse program, we also build a correct loop condition that guarantees the loop in the reverse program has the same number of iterations at runtime as the loop in the original program.







The Backstroke compiler is implemented based on the open-source ROSE compiler \cite{quinlan2000rose} for C++ programming language. 
Specifically, Backstroke is a subproject of ROSE and it can be downloaded from http://www.rosecompiler.org (Backstroke is located at ROSE/projects/backstroke).

\paragraph{Future work}
We think there are two directions to the future work: the first one includes eliminating restrictions in our method; the second one is building a framework for program inversion cooperating with our compiler.

We could expand the scope of the target language by eliminating restrictions in our method.
One main limitation is that we cannot handle programs with aliasing.
Since our method heavily depends on SSA, we may need an SSA representation to resolve aliases in the program \cite{cytron1993efficient}.
In addition, we could also try to handle linked data structures. 
We have seen previously that the method to handle arrays can be applied to linked data structures, but more informations are required from more sophisticated analyses.
For example, from shape analysis, we may detect a pattern of linked list traversal, and then we know each list node is traversed only once - an important information that is very useful.
Another limitation of our method is that we lack of ability to solve equations to retrieve some values. 
For example, assume we have $a = x + y$ and $ b = x - y$, then by solving equations we could get $x$ and $y$ from $a$ and $b$ through $x = (a + b) / 2$ and $y = (a - b) / 2$.
How to combine this capability to our VSG is a potential problem to be solved.

In Chapter \ref{chapter:cpp}, we have seen that some high level informations are helpful for Backstroke to generate better result.
Also, we have also seen that to properly perform state savings on C++ objects, we need three new interfaces for us to do the clone, copy and destroy. 
This suggests a framework for program inversion, in which some constructs are provided to implement some necessary functions, or give the compiler some hints to generate better result (think about how we handle C++ STL containers). 
From another aspect, this new framework provides new language features that could be recognized by Backstroke.
We believe that this new framework (or new language) together with Backstroke can produce much better results for program inversion.






\bibliography{biblib2,more2,library2,biblib,more,library}

\end{document}
